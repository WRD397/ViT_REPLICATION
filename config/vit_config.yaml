project : "VIT_EXPLORATION"
run : "vittinyv3_tinyimagenet200_runV9"
run_notes : "inc model capacity, lowering regularization as the model has underfitting issue." 
wandb_tags : ["ViT", "TINYIMAGENET200", "custom model", "layerscale_eps", "class_token_dropout"]

data:
  CIFAR10:
    dataset : 'CIFAR10'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10'
    channels: 3
    num_workers: 8
    mean_aug: [0.4914, 0.4822, 0.4465]
    std_aug: [0.2023, 0.1994, 0.2010]
    num_classes: 10
  CIFAR100:
    dataset : 'CIFAR100'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR100'
    channels: 3
    num_workers: 8
    mean_aug: [0.5071, 0.4867, 0.4408]
    std_aug: [0.2675, 0.2565, 0.2761]
    num_classes: 100
  TINYIMAGENET200:
    dataset : 'TINYIMAGENET200'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200'
    channels: 3
    num_workers: 8
    mean_aug: [0.485, 0.456, 0.406]
    std_aug: [0.229, 0.224, 0.225]
    num_classes: 200
  CALTECH256:
    dataset: 'CALTECH256'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CALTECH256'
    channels: 3
    num_workers: 8
    mean_aug: [0.5531906485557556, 0.5342377424240112, 0.5071621537208557]
    std_aug: [0.23687367141246796, 0.2358572781085968, 0.2385127693414688]
    num_classes: 256

model:
  # test model
  VIT_TINYV0:
    name: 'vit_tiny_v0'
    patch_size: 4
    emb_size: 32 # 768 in original paper
    depth: 2 # 12 in original paper
    num_heads: 2 # 12 in original paper
    mlp_ratio: 1.0 # orignial paper 4.0
    dropout: 0.0

  # tiny - so far successful  
  VIT_TINYV1:
    name: 'vit_tiny_v1'
    patch_size: 4
    emb_size: 512 # 768 in original paper
    depth: 10 # 12 in original paper
    num_heads: 8 # 12 in original paper
    mlp_ratio: 3.0 # orignial paper 4.0
    dropout: 0.2

  # tiny - for testing the impact of augmentations
  VIT_TINYV2:
    name: 'vit_tiny_v2'
    patch_size: 4
    emb_size: 384 # 768 in original paper
    depth: 8 # 12 in original paper
    num_heads: 8 # 12 in original paper
    mlp_ratio: 3.0 # orignial paper 4.0
    dropout: 0.2

  # tiny - using custom attention
  VIT_TINYV3:
    name: 'vit_tiny_v3'
    patch_size: 8
    emb_size: 384
    depth: 12
    num_heads: 6
    mlp_ratio: 4.0 
    dropout: 0.1
    attention_drop: 0.1
    projection_drop: 0.1
    drop_path_rate: 0.2
    qkv_bias: True
    layerscale_eps : 0.01 #0.1 gave the best result in caltech256 subset - 1 means disabling the impact
    class_token_dropout : 0.5 #0.3 gave the best result in caltech256 subset test - 0 means disabling the impact

  # tiny - custom attention - for deeper training
  VIT_TINYV4:
    name: 'vit_tiny_v4'
    patch_size: 16
    emb_size: 384
    depth: 12
    num_heads: 6
    mlp_ratio: 4.0 
    dropout: 0.0
    attention_drop: 0.0
    projection_drop: 0.0
    drop_path_rate: 0.1
    qkv_bias: True
    layerscale_eps : 0.05 #0.1 gave the best result in caltech256 subset - 1 means disabling the impact
    class_token_dropout : 0.3 #0.3 gave the best result in caltech256 subset test - 0 means disabling the impact

training:
  mixup:
    enabled: true
    mixup_alpha: 0.1
    cutmix_alpha: 0.6
    label_smoothing_mixup: 0.2
  epochs: 200
  lr: 0.001
  weight_decay: 0.03  #0.1 is used in original paper. 0.1 is very high decay - used to control overfit.
  scheduler : false
  scheduler_warmup : true
  warmup_steps : 30
  label_smoothing_enabled : false
  label_smoothing : 0.0 # increase to reduce overfitting
  es_patience : 30
  es_improv_delta : 0.001
  augmentation_enabled : true
  apply_class_balance : true
  num_subset_class : 200
  num_subset_sample : 200
  batch_size : 128
  image_size : 224

training_dummy:
  mixup:
    enabled: false
    mixup_alpha: 0.0
    cutmix_alpha: 0.0
    label_smoothing_mixup: 0.0
  epochs: 50
  lr: 0.0003
  weight_decay: 0.0 # 0.1 is used in original paper. 0.1 is very high decay - used to control overfit.
  scheduler_warmup : true
  warmup_steps : 30
  label_smoothing_enabled : false
  label_smoothing : 0.0 # increase to reduce overfitting
  es_patience : 30
  es_improv_delta : 0.001
  augmentation_enabled : false
  apply_class_balance : true
  num_subset_class : 10
  num_subset_sample : 80
  batch_size : 128
  image_size : 64

## ********* RESOURCES ***********

### code snippet for calculating the mean/std deviation from the dataset.
# from torchvision import datasets, transforms
# from torch.utils.data import DataLoader
# import torch

# dataset = datasets.CIFAR100(
#     root='./data', train=True, download=True,
#     transform=transforms.ToTensor())

# loader = DataLoader(dataset, batch_size=50000, shuffle=False)
# data = next(iter(loader))[0]

# mean = data.mean(dim=(0, 2, 3))
# std = data.std(dim=(0, 2, 3))

# print("Mean:", mean)
# print("Std:", std)

# CIFAR-100 has 100 classes, so:
# Random guessing gives ~1% accuracy.
# Just repeating a single class gets you ~1%.
# A shallow CNN should hit 25–35% in <20 epochs.
# A small ViT should get at least 15–30% accuracy by epoch 20 with reasonable hyperparameters.