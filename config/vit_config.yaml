data:
  CIFAR10:
    dataset : 'CIFAR10'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10'
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 32
    mean_aug: [0.4914, 0.4822, 0.4465]
    std_aug: [0.2023, 0.1994, 0.2010]
    num_classes: 10
  CIFAR100:
    dataset : 'CIFAR100'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR100'
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 32
    mean_aug: [0.5071, 0.4867, 0.4408]
    std_aug: [0.2675, 0.2565, 0.2761]
    num_classes: 100
  TINYIMAGENET:
    dataset : 'TINYIMAGENET'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET'
    subset_enabled : true
    subset_size : 50000 # 10% of total(100k training data)
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 64
    mean_aug: [0.485, 0.456, 0.406]
    std_aug: [0.229, 0.224, 0.225]
    num_classes: 200

model:
  VIT_SMALL:
    name: 'vit_small'
    patch_size: 4
    emb_size: 512 # 768 in original paper
    depth: 10 # 12 in original paper
    num_heads: 8 # 12 in original paper
    mlp_ratio: 2.0 # orignial paper 4.0
    dropout: 0.2

training:
  mixup:
    enabled: true
    mixup_alpha: 0.0
    cutmix_alpha: 0.0
    label_smoothing_mixup: 0.1
  epochs: 200
  lr: 0.0003
  weight_decay: 0.01 # 0.1 is used in original paper. 0.1 is very high decay - used to control overfit.
  scheduler : false
  scheduler_warmup : true
  warmup_steps : 30
  label_smoothing_enabled : false
  label_smoothing : 0.2 # increase to reduce overfitting



## ********* RESOURCES ***********

### code snippet for calculating the mean/std deviation from the dataset.
# from torchvision import datasets, transforms
# from torch.utils.data import DataLoader
# import torch

# dataset = datasets.CIFAR100(
#     root='./data', train=True, download=True,
#     transform=transforms.ToTensor())

# loader = DataLoader(dataset, batch_size=50000, shuffle=False)
# data = next(iter(loader))[0]

# mean = data.mean(dim=(0, 2, 3))
# std = data.std(dim=(0, 2, 3))

# print("Mean:", mean)
# print("Std:", std)

# CIFAR-100 has 100 classes, so:
# Random guessing gives ~1% accuracy.
# Just repeating a single class gets you ~1%.
# A shallow CNN should hit 25–35% in <20 epochs.
# A small ViT should get at least 15–30% accuracy by epoch 20 with reasonable hyperparameters.