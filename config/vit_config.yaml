project : "VIT_EXPLORATION"
run : "vittinyv1_cifar10_runV1"
run_notes : "Epoch100 - running the tested version of vityTinyV1 on cifar10. it has succeeded with slight overfitting last time."
wandb_tags : ["ViT", "CIFAR10", "no mixup"]

data:
  CIFAR10:
    dataset : 'CIFAR10'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10'
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 32
    mean_aug: [0.4914, 0.4822, 0.4465]
    std_aug: [0.2023, 0.1994, 0.2010]
    num_classes: 10
  CIFAR100:
    dataset : 'CIFAR100'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR100'
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 32
    mean_aug: [0.5071, 0.4867, 0.4408]
    std_aug: [0.2675, 0.2565, 0.2761]
    num_classes: 100
  TINYIMAGENET200:
    dataset : 'TINYIMAGENET200'
    data_path: '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET'
    subset_enabled : true
    subset_size : 50000 # 50% of total(100k training data)
    channels: 3
    batch_size: 128
    num_workers: 8
    img_size: 64
    mean_aug: [0.485, 0.456, 0.406]
    std_aug: [0.229, 0.224, 0.225]
    num_classes: 200

model:
  VIT_TINYV0:
    name: 'vit_tiny_v0'
    patch_size: 4
    emb_size: 64 # 768 in original paper
    depth: 4 # 12 in original paper
    num_heads: 2 # 12 in original paper
    mlp_ratio: 1.0 # orignial paper 4.0
    dropout: 0.1
  
  VIT_TINYV1:
    name: 'vit_tiny_v1'
    patch_size: 4
    emb_size: 512 # 768 in original paper
    depth: 10 # 12 in original paper
    num_heads: 8 # 12 in original paper
    mlp_ratio: 3.0 # orignial paper 4.0
    dropout: 0.2

training:
  mixup:
    enabled: false
    mixup_alpha: 0.1
    cutmix_alpha: 0.01
    label_smoothing_mixup: 0.1
  epochs: 100
  lr: 0.0003
  weight_decay: 0.008 # 0.1 is used in original paper. 0.1 is very high decay - used to control overfit.
  scheduler : false
  scheduler_warmup : true
  warmup_steps : 20
  label_smoothing_enabled : true
  label_smoothing : 0.2 # increase to reduce overfitting
  es_patience : 10
  es_improv_delta : 0.001


## ********* RESOURCES ***********

### code snippet for calculating the mean/std deviation from the dataset.
# from torchvision import datasets, transforms
# from torch.utils.data import DataLoader
# import torch

# dataset = datasets.CIFAR100(
#     root='./data', train=True, download=True,
#     transform=transforms.ToTensor())

# loader = DataLoader(dataset, batch_size=50000, shuffle=False)
# data = next(iter(loader))[0]

# mean = data.mean(dim=(0, 2, 3))
# std = data.std(dim=(0, 2, 3))

# print("Mean:", mean)
# print("Std:", std)

# CIFAR-100 has 100 classes, so:
# Random guessing gives ~1% accuracy.
# Just repeating a single class gets you ~1%.
# A shallow CNN should hit 25–35% in <20 epochs.
# A small ViT should get at least 15–30% accuracy by epoch 20 with reasonable hyperparameters.