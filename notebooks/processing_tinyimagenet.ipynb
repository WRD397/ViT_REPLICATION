{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0a443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH)) \n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "DESTINATION_PATH =f'{ROOT_DIR_PATH}/data/TINYIMAGENET/'\n",
    "ZIP_NAME = 'tiny-imagenet-200.zip'\n",
    "\n",
    "\n",
    "def download_tiny_imagenet(save_path=f'{DESTINATION_PATH}{ZIP_NAME}'):\n",
    "    url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "    print(f\"Downloading TinyImageNet to {save_path}...\")\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(zip_path=f'{DESTINATION_PATH}{ZIP_NAME}', extract_to=DESTINATION_PATH):\n",
    "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val'):\n",
    "    print(f\"Reorganizing validation folder at {base_dir}...\")\n",
    "    img_dir = os.path.join(base_dir, 'images')\n",
    "    ann_file = os.path.join(base_dir, 'val_annotations.txt')\n",
    "\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            file_name, class_name = line.split('\\t')[:2]\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            src = os.path.join(img_dir, file_name)\n",
    "            dst = os.path.join(class_dir, file_name)\n",
    "            if os.path.exists(src):\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "    shutil.rmtree(img_dir)\n",
    "    print(\"Validation images reorganized!\")\n",
    "\n",
    "def main():\n",
    "    dst_dir = DESTINATION_PATH\n",
    "    zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\"\n",
    "\n",
    "    if not os.path.isdir(dst_dir):\n",
    "        print('creating the destination dir. & downloading')\n",
    "        os.makedirs(os.path.dirname(dst_dir), exist_ok=True)\n",
    "        download_tiny_imagenet(zip_fname)\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val')\n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}/\")\n",
    "    else:\n",
    "        print(f\"Dataset directory {dst_dir} already exists, zip downloaded.\")\n",
    "\n",
    "    if not os.path.isdir(f'{DESTINATION_PATH}tiny-imagenet-200/'):\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val')\n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}\")\n",
    "    else : print(f'TinyImageNet already exists under: {dst_dir}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac15fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = DESTINATION_PATH\n",
    "zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57cf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the destination dir. & downloading\n",
      "Downloading TinyImageNet to /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tinyimagenet200.zip...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ already exists. Skipping download.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreating the destination dir. & downloading\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dst_dir), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdownload_tiny_imagenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m extract_dataset(zip_path\u001b[38;5;241m=\u001b[39mzip_fname, extract_to\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dst_dir))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyImageNet is ready under: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mdownload_tiny_imagenet\u001b[0;34m(save_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://cs231n.stanford.edu/tiny-imagenet-200.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TinyImageNet to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # You can customize this path\n",
    "    dst_dir = DESTINATION_PATH\n",
    "    zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\"\n",
    "\n",
    "    if not os.path.isdir(dst_dir):\n",
    "        print('creating the destination dir. & downloading')\n",
    "        os.makedirs(os.path.dirname(dst_dir), exist_ok=True)\n",
    "        download_tiny_imagenet(zip_fname)\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        \n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}/\")\n",
    "    else:\n",
    "        print(f\"Dataset directory {dst_dir}/ already exists. Skipping download.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_val_folder(base_dir=f'{EXTRACT_PATH}/tiny-imagenet-200/val'):\n",
    "    print(f\"Reorganizing validation folder at {base_dir}...\")\n",
    "    img_dir = os.path.join(base_dir, 'images')\n",
    "    ann_file = os.path.join(base_dir, 'val_annotations.txt')\n",
    "\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            file_name, class_name = line.split('\\t')[:2]\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            src = os.path.join(img_dir, file_name)\n",
    "            dst = os.path.join(class_dir, file_name)\n",
    "            if os.path.exists(src):\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "    shutil.rmtree(img_dir)\n",
    "    print(\"Validation images reorganized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba372649",
   "metadata": {},
   "source": [
    "# training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e7dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model.vit import VisionTransformerSmall\n",
    "from utils.model_io import save_model\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetName, nvmlShutdown,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetUtilizationRates\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from timm.data import Mixup\n",
    "import numpy as np\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, \n",
    "                    mixup_fn=None, scheduler_warmup_enabled=False, scheduler_warmup=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=\"Training\", leave=True)\n",
    "    for  inputs, targets in progress_bar:\n",
    "        #print(f'input shape : {inputs.shape}, taget_shape : {targets.shape}, target dim : {targets.ndim}')\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if mixup_fn is not None:\n",
    "            inputs, targets = mixup_fn(inputs, targets)\n",
    "\n",
    "        if targets.ndim == 2:\n",
    "            targets = targets.type_as(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler_warmup_enabled:\n",
    "            if scheduler_warmup is None : raise Exception(f'scheduler warmup is enabled, but no scheduler object has been passed in train_one_epoch function')\n",
    "            scheduler_warmup.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if targets.ndim == 2:\n",
    "            # MixUp with soft labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            _, true_classes = targets.max(1)  # Take argmax of soft labels as true class\n",
    "            correct += predicted.eq(true_classes).sum().item()\n",
    "            total += targets.size(0)\n",
    "        else :\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        # Update progress bar with metrics\n",
    "        if total > 0:\n",
    "            avg_loss = running_loss / total\n",
    "            accuracy = 100. * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                \"Acc\": f\"{accuracy:.2f}%\"\n",
    "            })\n",
    "\n",
    "        else : raise Exception(f'Expected non-zero batch size, but got 0 targets. Check if the dataset is empty or DataLoader is misconfigured.')\n",
    "\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Validation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            # Compute accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Avoid division by zero on first step\n",
    "            if total > 0:\n",
    "                avg_loss = running_loss / total\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"Acc\": f\"{accuracy:.2f}%\"\n",
    "                })\n",
    "                \n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd53924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load config\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "# loading cifar100\n",
    "#cifar100_config = config[\"data\"]['CIFAR100']\n",
    "dataset_config = config[\"data\"]['TINYIMAGENET']\n",
    "DATASET = dataset_config[\"dataset\"]\n",
    "DATA_DIR = dataset_config[\"data_path\"]\n",
    "BATCH = dataset_config[\"batch_size\"]\n",
    "NUM_WORKERS = dataset_config[\"num_workers\"]\n",
    "IMAGE = dataset_config[\"img_size\"]\n",
    "NUM_CLASSES = dataset_config[\"num_classes\"]\n",
    "CHANNELS = dataset_config[\"channels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b501df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset : TINYIMAGENET\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "Train batches: 782, Validation batches: 79\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([128, 3, 64, 64]), torch.Size([128])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "print(f'loading dataset : {DATASET}')\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, val_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf6678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee826c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b497e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dbd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
