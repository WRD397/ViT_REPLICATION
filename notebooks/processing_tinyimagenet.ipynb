{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0a443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH)) \n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "DESTINATION_PATH =f'{ROOT_DIR_PATH}/data/TINYIMAGENET/'\n",
    "ZIP_NAME = 'tiny-imagenet-200.zip'\n",
    "\n",
    "\n",
    "def download_tiny_imagenet(save_path=f'{DESTINATION_PATH}{ZIP_NAME}'):\n",
    "    url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "    print(f\"Downloading TinyImageNet to {save_path}...\")\n",
    "    urllib.request.urlretrieve(url, save_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(zip_path=f'{DESTINATION_PATH}{ZIP_NAME}', extract_to=DESTINATION_PATH):\n",
    "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val'):\n",
    "    print(f\"Reorganizing validation folder at {base_dir}...\")\n",
    "    img_dir = os.path.join(base_dir, 'images')\n",
    "    ann_file = os.path.join(base_dir, 'val_annotations.txt')\n",
    "\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            file_name, class_name = line.split('\\t')[:2]\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            src = os.path.join(img_dir, file_name)\n",
    "            dst = os.path.join(class_dir, file_name)\n",
    "            if os.path.exists(src):\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "    shutil.rmtree(img_dir)\n",
    "    print(\"Validation images reorganized!\")\n",
    "\n",
    "def main():\n",
    "    dst_dir = DESTINATION_PATH\n",
    "    zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\"\n",
    "\n",
    "    if not os.path.isdir(dst_dir):\n",
    "        print('creating the destination dir. & downloading')\n",
    "        os.makedirs(os.path.dirname(dst_dir), exist_ok=True)\n",
    "        download_tiny_imagenet(zip_fname)\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val')\n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}/\")\n",
    "    else:\n",
    "        print(f\"Dataset directory {dst_dir} already exists, zip downloaded.\")\n",
    "\n",
    "    if not os.path.isdir(f'{DESTINATION_PATH}tiny-imagenet-200/'):\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        rearrange_val_folder(base_dir=f'{DESTINATION_PATH}tiny-imagenet-200/val')\n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}\")\n",
    "    else : print(f'TinyImageNet already exists under: {dst_dir}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac15fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = DESTINATION_PATH\n",
    "zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b57cf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the destination dir. & downloading\n",
      "Downloading TinyImageNet to /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tinyimagenet200.zip...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ already exists. Skipping download.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreating the destination dir. & downloading\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dst_dir), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdownload_tiny_imagenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m extract_dataset(zip_path\u001b[38;5;241m=\u001b[39mzip_fname, extract_to\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(dst_dir))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyImageNet is ready under: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mdownload_tiny_imagenet\u001b[0;34m(save_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://cs231n.stanford.edu/tiny-imagenet-200.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TinyImageNet to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # You can customize this path\n",
    "    dst_dir = DESTINATION_PATH\n",
    "    zip_fname = f\"{DESTINATION_PATH}{ZIP_NAME}\"\n",
    "\n",
    "    if not os.path.isdir(dst_dir):\n",
    "        print('creating the destination dir. & downloading')\n",
    "        os.makedirs(os.path.dirname(dst_dir), exist_ok=True)\n",
    "        download_tiny_imagenet(zip_fname)\n",
    "        extract_dataset(zip_path=zip_fname, extract_to=os.path.dirname(dst_dir))\n",
    "        \n",
    "        print(f\"TinyImageNet is ready under: {dst_dir}/\")\n",
    "    else:\n",
    "        print(f\"Dataset directory {dst_dir}/ already exists. Skipping download.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_val_folder(base_dir=f'{EXTRACT_PATH}/tiny-imagenet-200/val'):\n",
    "    print(f\"Reorganizing validation folder at {base_dir}...\")\n",
    "    img_dir = os.path.join(base_dir, 'images')\n",
    "    ann_file = os.path.join(base_dir, 'val_annotations.txt')\n",
    "\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            file_name, class_name = line.split('\\t')[:2]\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            src = os.path.join(img_dir, file_name)\n",
    "            dst = os.path.join(class_dir, file_name)\n",
    "            if os.path.exists(src):\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "    shutil.rmtree(img_dir)\n",
    "    print(\"Validation images reorganized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba372649",
   "metadata": {},
   "source": [
    "# training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e7dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model.vit import VisionTransformerSmall\n",
    "from utils.model_io import save_model\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetName, nvmlShutdown,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetUtilizationRates\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from timm.data import Mixup\n",
    "import numpy as np\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, \n",
    "                    mixup_fn=None, scheduler_warmup_enabled=False, scheduler_warmup=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=\"Training\", leave=True)\n",
    "    for  inputs, targets in progress_bar:\n",
    "        #print(f'input shape : {inputs.shape}, taget_shape : {targets.shape}, target dim : {targets.ndim}')\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if mixup_fn is not None:\n",
    "            inputs, targets = mixup_fn(inputs, targets)\n",
    "\n",
    "        if targets.ndim == 2:\n",
    "            targets = targets.type_as(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler_warmup_enabled:\n",
    "            if scheduler_warmup is None : raise Exception(f'scheduler warmup is enabled, but no scheduler object has been passed in train_one_epoch function')\n",
    "            scheduler_warmup.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if targets.ndim == 2:\n",
    "            # MixUp with soft labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            _, true_classes = targets.max(1)  # Take argmax of soft labels as true class\n",
    "            correct += predicted.eq(true_classes).sum().item()\n",
    "            total += targets.size(0)\n",
    "        else :\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        # Update progress bar with metrics\n",
    "        if total > 0:\n",
    "            avg_loss = running_loss / total\n",
    "            accuracy = 100. * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                \"Acc\": f\"{accuracy:.2f}%\"\n",
    "            })\n",
    "\n",
    "        else : raise Exception(f'Expected non-zero batch size, but got 0 targets. Check if the dataset is empty or DataLoader is misconfigured.')\n",
    "\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Validation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            # Compute accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Avoid division by zero on first step\n",
    "            if total > 0:\n",
    "                avg_loss = running_loss / total\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"Acc\": f\"{accuracy:.2f}%\"\n",
    "                })\n",
    "                \n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd53924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load config\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "# loading cifar100\n",
    "#cifar100_config = config[\"data\"]['CIFAR100']\n",
    "dataset_config = config[\"data\"]['TINYIMAGENET']\n",
    "DATASET = dataset_config[\"dataset\"]\n",
    "DATA_DIR = dataset_config[\"data_path\"]\n",
    "BATCH = dataset_config[\"batch_size\"]\n",
    "NUM_WORKERS = dataset_config[\"num_workers\"]\n",
    "IMAGE = dataset_config[\"img_size\"]\n",
    "NUM_CLASSES = dataset_config[\"num_classes\"]\n",
    "CHANNELS = dataset_config[\"channels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b501df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset : TINYIMAGENET\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "training size  : 50000\n",
      "validation size : 10000\n",
      "Classes: 200\n",
      "Sample label: 75\n",
      "Train batches: 391, Validation batches: 79\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([128, 3, 64, 64]), torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "print(f'loading dataset : {DATASET}')\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, val_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16332e90",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed890390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading dataset : TINYIMAGENET\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/\n",
      "training size  : 50000\n",
      "validation size : 10000\n",
      "Classes: 200\n",
      "Sample label: 62\n",
      "Train batches: 391, Validation batches: 79\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([128, 3, 64, 64]), torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load config\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "# loading cifar100\n",
    "#cifar100_config = config[\"data\"]['CIFAR100']\n",
    "dataset_config = config[\"data\"]['TINYIMAGENET']\n",
    "DATASET = dataset_config[\"dataset\"]\n",
    "DATA_DIR = dataset_config[\"data_path\"]\n",
    "BATCH = dataset_config[\"batch_size\"]\n",
    "NUM_WORKERS = dataset_config[\"num_workers\"]\n",
    "IMAGE = dataset_config[\"img_size\"]\n",
    "NUM_CLASSES = dataset_config[\"num_classes\"]\n",
    "CHANNELS = dataset_config[\"channels\"]\n",
    "\n",
    "# loading data\n",
    "print(f'loading dataset : {DATASET}')\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, val_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e683bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model.vit import VisionTransformerSmall\n",
    "from utils.model_io import save_model\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "from pynvml import (\n",
    "    nvmlInit, nvmlDeviceGetName, nvmlShutdown,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetUtilizationRates\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from timm.data import Mixup\n",
    "import numpy as np\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edf6678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load ROOT_PATH from .env\n",
    "load_dotenv()\n",
    "ROOT_DIR = os.environ.get(\"ROOT_PATH\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\", \"TINYIMAGENET\")\n",
    "\n",
    "# Define clean (deterministic) transform\n",
    "clean_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Adjust to your ViT input size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Recreate the clean version of the dataset\n",
    "clean_dataset = datasets.ImageFolder(\n",
    "    root=os.path.join(DATA_DIR, \"tiny-imagenet-200\", \"train\"),\n",
    "    transform=clean_transform\n",
    ")\n",
    "\n",
    "# Pick a fixed index\n",
    "index = 24234\n",
    "img, label = clean_dataset[index]\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(f\"Class: {clean_dataset.classes[label]} (Label Index: {label})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ec4174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaee826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loss: 5.3892\n",
      "Predicted label distribution: tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
      "       device='cuda:0')\n",
      "True label distribution     : tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "modelConfig = config[\"model\"]\n",
    "specific_config = modelConfig['VIT_SMALL']\n",
    "MODEL_NAME = specific_config[\"name\"]\n",
    "modelConfigDict = {\n",
    "    'CHANNEL' : CHANNELS,\n",
    "    'PATCH' : specific_config['patch_size'],\n",
    "    'EMBEDDING' : specific_config['emb_size'],\n",
    "    'IMAGE' : IMAGE,\n",
    "    'NUM_HEADS' : specific_config['num_heads'],\n",
    "    'MLP_RATIO' : specific_config['mlp_ratio'],\n",
    "    'DROPOUT' : specific_config['dropout'],\n",
    "    'NUM_CLASSES' : NUM_CLASSES,\n",
    "    'DEPTH' : specific_config['depth']\n",
    "}    \n",
    "# 4. Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#model = VisionTransformerSmall(**modelConfigDict).to(device)\n",
    "\n",
    "# tryin with different model\n",
    "import torchvision.models as models\n",
    "model = models.resnet18(pretrained=True)\n",
    "# Replace classifier head to match TinyImageNet classes (200)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    #transforms.Resize((IMAGE, IMAGE)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(dataset_config['mean_aug'], dataset_config['std_aug'])\n",
    "])\n",
    "# 7. Load clean dataset (few samples)\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=os.path.join(DATA_DIR, \"tiny-imagenet-200\", \"train\"),\n",
    "    transform=simple_transform\n",
    ")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 8. Run model on a small batch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "images, labels = next(iter(loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    preds = outputs.argmax(dim=1)\n",
    "\n",
    "# 9. Print outputs\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(\"Predicted label distribution:\", torch.bincount(preds))\n",
    "print(\"True label distribution     :\", torch.bincount(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d5e9559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1179, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04b497e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 200\n",
      "Sample class names: ['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640']\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"tiny-imagenet-200\", \"train\"))\n",
    "print(f\"Number of classes: {len(dataset.classes)}\")\n",
    "print(\"Sample class names:\", dataset.classes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913dbd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500, 8: 500, 9: 500, 10: 500, 11: 500, 12: 500, 13: 500, 14: 500, 15: 500, 16: 500, 17: 500, 18: 500, 19: 500, 20: 500, 21: 500, 22: 500, 23: 500, 24: 500, 25: 500, 26: 500, 27: 500, 28: 500, 29: 500, 30: 500, 31: 500, 32: 500, 33: 500, 34: 500, 35: 500, 36: 500, 37: 500, 38: 500, 39: 500, 40: 500, 41: 500, 42: 500, 43: 500, 44: 500, 45: 500, 46: 500, 47: 500, 48: 500, 49: 500, 50: 500, 51: 500, 52: 500, 53: 500, 54: 500, 55: 500, 56: 500, 57: 500, 58: 500, 59: 500, 60: 500, 61: 500, 62: 500, 63: 500, 64: 500, 65: 500, 66: 500, 67: 500, 68: 500, 69: 500, 70: 500, 71: 500, 72: 500, 73: 500, 74: 500, 75: 500, 76: 500, 77: 500, 78: 500, 79: 500, 80: 500, 81: 500, 82: 500, 83: 500, 84: 500, 85: 500, 86: 500, 87: 500, 88: 500, 89: 500, 90: 500, 91: 500, 92: 500, 93: 500, 94: 500, 95: 500, 96: 500, 97: 500, 98: 500, 99: 500, 100: 500, 101: 500, 102: 500, 103: 500, 104: 500, 105: 500, 106: 500, 107: 500, 108: 500, 109: 500, 110: 500, 111: 500, 112: 500, 113: 500, 114: 500, 115: 500, 116: 500, 117: 500, 118: 500, 119: 500, 120: 500, 121: 500, 122: 500, 123: 500, 124: 500, 125: 500, 126: 500, 127: 500, 128: 500, 129: 500, 130: 500, 131: 500, 132: 500, 133: 500, 134: 500, 135: 500, 136: 500, 137: 500, 138: 500, 139: 500, 140: 500, 141: 500, 142: 500, 143: 500, 144: 500, 145: 500, 146: 500, 147: 500, 148: 500, 149: 500, 150: 500, 151: 500, 152: 500, 153: 500, 154: 500, 155: 500, 156: 500, 157: 500, 158: 500, 159: 500, 160: 500, 161: 500, 162: 500, 163: 500, 164: 500, 165: 500, 166: 500, 167: 500, 168: 500, 169: 500, 170: 500, 171: 500, 172: 500, 173: 500, 174: 500, 175: 500, 176: 500, 177: 500, 178: 500, 179: 500, 180: 500, 181: 500, 182: 500, 183: 500, 184: 500, 185: 500, 186: 500, 187: 500, 188: 500, 189: 500, 190: 500, 191: 500, 192: 500, 193: 500, 194: 500, 195: 500, 196: 500, 197: 500, 198: 500, 199: 500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels = [label for _, label in dataset]\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f64fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total class folders: 200\n",
      "Sample folders: ['n01917289', 'n02132136', 'n01983481', 'n01774384', 'n03804744']\n",
      "Sample image file from class n01917289:\n",
      "No image found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, \"tiny-imagenet-200\", \"train\")\n",
    "class_folders = os.listdir(train_dir)\n",
    "print(f\"Total class folders: {len(class_folders)}\")\n",
    "print(\"Sample folders:\", class_folders[:5])\n",
    "\n",
    "# Check sample image path inside first class\n",
    "sample_images = glob(os.path.join(train_dir, class_folders[0], \"images\", \"*.JPEG\"))\n",
    "print(f\"Sample image file from class {class_folders[0]}:\")\n",
    "print(sample_images[0] if sample_images else \"No image found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05382b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff3c14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tiny-imagenet-200/train\n",
      "  Total class folders: 200\n",
      "  Total image files:  100000\n",
      "----------------------------------------\n",
      "Path: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tiny-imagenet-200/val\n",
      "  Total class folders: 200\n",
      "  Total image files:  10000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 10000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_folders_and_files(root_dir):\n",
    "    class_folders = [d for d in os.listdir(root_dir)\n",
    "                     if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    \n",
    "    total_classes = len(class_folders)\n",
    "    total_images = 0\n",
    "\n",
    "    for class_folder in class_folders:\n",
    "        class_path = os.path.join(root_dir, class_folder)\n",
    "        image_files = [f for f in os.listdir(class_path)\n",
    "                       if os.path.isfile(os.path.join(class_path, f)) and f.endswith('.JPEG')]\n",
    "        total_images += len(image_files)\n",
    "    \n",
    "    print(f\"Path: {root_dir}\")\n",
    "    print(f\"  Total class folders: {total_classes}\")\n",
    "    print(f\"  Total image files:  {total_images}\")\n",
    "    print(\"-\" * 40)\n",
    "    return total_classes, total_images\n",
    "\n",
    "\n",
    "#  Run this to verify structure:\n",
    "train_path = '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tiny-imagenet-200/train'\n",
    "val_path = '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET/tiny-imagenet-200/val'\n",
    "\n",
    "count_folders_and_files(train_path)\n",
    "count_folders_and_files(val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ad0e0",
   "metadata": {},
   "source": [
    "### 2 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] Loss: 3.5236, Train Acc: 5.00%\n",
      "Epoch [2/25] Loss: 2.8097, Train Acc: 14.00%\n",
      "Epoch [3/25] Loss: 2.6212, Train Acc: 29.00%\n",
      "Epoch [4/25] Loss: 2.2390, Train Acc: 37.00%\n",
      "Epoch [5/25] Loss: 1.6952, Train Acc: 54.00%\n",
      "Epoch [6/25] Loss: 1.1649, Train Acc: 66.00%\n",
      "Epoch [7/25] Loss: 0.8610, Train Acc: 71.00%\n",
      "Epoch [8/25] Loss: 0.4986, Train Acc: 88.00%\n",
      "Epoch [9/25] Loss: 0.3869, Train Acc: 93.00%\n",
      "Epoch [10/25] Loss: 0.1977, Train Acc: 98.00%\n",
      "Epoch [11/25] Loss: 0.0854, Train Acc: 100.00%\n",
      "Epoch [12/25] Loss: 0.0496, Train Acc: 99.00%\n",
      "Epoch [13/25] Loss: 0.0229, Train Acc: 100.00%\n",
      "Epoch [14/25] Loss: 0.0283, Train Acc: 99.00%\n",
      "Epoch [15/25] Loss: 0.0086, Train Acc: 100.00%\n",
      "Epoch [16/25] Loss: 0.0057, Train Acc: 100.00%\n",
      "Epoch [17/25] Loss: 0.0034, Train Acc: 100.00%\n",
      "Epoch [18/25] Loss: 0.0025, Train Acc: 100.00%\n",
      "Epoch [19/25] Loss: 0.0022, Train Acc: 100.00%\n",
      "Epoch [20/25] Loss: 0.0017, Train Acc: 100.00%\n",
      "Epoch [21/25] Loss: 0.0013, Train Acc: 100.00%\n",
      "Epoch [22/25] Loss: 0.0011, Train Acc: 100.00%\n",
      "Epoch [23/25] Loss: 0.0009, Train Acc: 100.00%\n",
      "Epoch [24/25] Loss: 0.0008, Train Acc: 100.00%\n",
      "Epoch [25/25] Loss: 0.0007, Train Acc: 100.00%\n",
      "\n",
      "Overfit test complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "DATA_PATH = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/tiny-imagenet-200/train\" \n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 20\n",
    "SAMPLES_PER_CLASS = 5\n",
    "EPOCHS = 25\n",
    "LR = 0.001\n",
    "IMG_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Basic Transform (no aug) ----------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # TinyImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------- Dataset and Balanced Subset ----------\n",
    "dataset = datasets.ImageFolder(DATA_PATH, transform=transform)\n",
    "\n",
    "# Build class-balanced index list\n",
    "class_to_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(dataset):\n",
    "    class_to_indices[label].append(idx)\n",
    "\n",
    "subset_indices = []\n",
    "for label in sorted(class_to_indices.keys())[:NUM_CLASSES]:\n",
    "    subset_indices.extend(class_to_indices[label][:SAMPLES_PER_CLASS])\n",
    "\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "dataloader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ---------- Simple 2-Layer CNN ----------\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B, 32, 32, 32)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (B, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B, 64, 16, 16)\n",
    "            nn.Flatten(),  # (B, 64*16*16)\n",
    "            nn.Linear(64 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ---------- Training Setup ----------\n",
    "model = TwoLayerCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = outputs.max(1)\n",
    "        total_correct += preds.eq(labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    acc = total_correct / total_samples * 100\n",
    "    avg_loss = running_loss / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\nOverfit test complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba3087",
   "metadata": {},
   "source": [
    "### ViT - timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8a64da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 5.00%\n",
      "Epoch 02 | Train Acc: 18.00%\n",
      "Epoch 03 | Train Acc: 26.00%\n",
      "Epoch 04 | Train Acc: 30.00%\n",
      "Epoch 05 | Train Acc: 29.00%\n",
      "Epoch 06 | Train Acc: 37.00%\n",
      "Epoch 07 | Train Acc: 37.00%\n",
      "Epoch 08 | Train Acc: 38.00%\n",
      "Epoch 09 | Train Acc: 41.00%\n",
      "Epoch 10 | Train Acc: 47.00%\n",
      "Epoch 11 | Train Acc: 54.00%\n",
      "Epoch 12 | Train Acc: 57.00%\n",
      "Epoch 13 | Train Acc: 58.00%\n",
      "Epoch 14 | Train Acc: 63.00%\n",
      "Epoch 15 | Train Acc: 62.00%\n",
      "Epoch 16 | Train Acc: 61.00%\n",
      "Epoch 17 | Train Acc: 71.00%\n",
      "Epoch 18 | Train Acc: 71.00%\n",
      "Epoch 19 | Train Acc: 78.00%\n",
      "Epoch 20 | Train Acc: 75.00%\n",
      "Epoch 21 | Train Acc: 74.00%\n",
      "Epoch 22 | Train Acc: 79.00%\n",
      "Epoch 23 | Train Acc: 79.00%\n",
      "Epoch 24 | Train Acc: 83.00%\n",
      "Epoch 25 | Train Acc: 86.00%\n",
      "Epoch 26 | Train Acc: 87.00%\n",
      "Epoch 27 | Train Acc: 89.00%\n",
      "Epoch 28 | Train Acc: 91.00%\n",
      "Epoch 29 | Train Acc: 92.00%\n",
      "Epoch 30 | Train Acc: 92.00%\n",
      "Epoch 31 | Train Acc: 96.00%\n",
      "Epoch 32 | Train Acc: 97.00%\n",
      "Epoch 33 | Train Acc: 99.00%\n",
      "Epoch 34 | Train Acc: 97.00%\n",
      "Epoch 35 | Train Acc: 98.00%\n",
      "Epoch 36 | Train Acc: 99.00%\n",
      "Epoch 37 | Train Acc: 99.00%\n",
      "Epoch 38 | Train Acc: 99.00%\n",
      "Epoch 39 | Train Acc: 99.00%\n",
      "Epoch 40 | Train Acc: 99.00%\n",
      "Epoch 41 | Train Acc: 99.00%\n",
      "Epoch 42 | Train Acc: 99.00%\n",
      "Epoch 43 | Train Acc: 99.00%\n",
      "Epoch 44 | Train Acc: 99.00%\n",
      "Epoch 45 | Train Acc: 100.00%\n",
      "Model has successfully overfitted the subset.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "# ========== Config ==========\n",
    "DATA_DIR = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/tiny-imagenet-200\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 100  # for 100 samples\n",
    "LR = 3e-4\n",
    "\n",
    "# ========== Transforms ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ========== Load Dataset ==========\n",
    "train_data = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "\n",
    "# Build class-balanced 100-sample subset: 20 classes  5 samples\n",
    "class_to_idx = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(train_data):\n",
    "    class_to_idx[label].append(idx)\n",
    "\n",
    "subset_indices = []\n",
    "for label in sorted(class_to_idx.keys())[:20]:  # 20 classes\n",
    "    subset_indices.extend(class_to_idx[label][:5])  # 5 samples each\n",
    "\n",
    "subset = Subset(train_data, subset_indices)\n",
    "loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# ========== Model ==========\n",
    "model = vit_tiny_patch16_224(pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)  # No weight decay\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc87ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d0d731",
   "metadata": {},
   "source": [
    "### ViT - own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd1c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)        # (B, emb_size, H/patch, W/patch)\n",
    "        x = x.flatten(2)              # (B, emb_size, N)\n",
    "        x = x.transpose(1, 2)         # (B, N, emb_size)\n",
    "        return x                      # Shape: (B, N, E)\n",
    "\n",
    "# Transformer Encoder Block - replicating the original Paper (Vaswani, 2017)\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class VisionTransformerSmall(nn.Module):\n",
    "    def __init__(self, \n",
    "                 CHANNEL, PATCH, EMBEDDING, IMAGE, NUM_HEADS, MLP_RATIO, DROPOUT, NUM_CLASSES, DEPTH\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.CHANNEL = CHANNEL\n",
    "        self.PATCH = PATCH\n",
    "        self.EMBEDDING = EMBEDDING\n",
    "        self.IMAGE = IMAGE\n",
    "        self.NUM_HEADS = NUM_HEADS\n",
    "        self.MLP_RATIO = MLP_RATIO\n",
    "        self.DROPOUT = DROPOUT\n",
    "        self.NUM_CLASS = NUM_CLASSES\n",
    "        self.DEPTH = DEPTH\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=self.CHANNEL,\n",
    "            patch_size=self.PATCH,\n",
    "            emb_size=self.EMBEDDING,\n",
    "            img_size=self.IMAGE\n",
    "        )\n",
    "\n",
    "        self.n_patches = (self.IMAGE// self.PATCH) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.EMBEDDING))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, self.EMBEDDING))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=self.EMBEDDING,\n",
    "                num_heads=self.NUM_HEADS,\n",
    "                mlp_ratio=self.MLP_RATIO,\n",
    "                dropout=self.DROPOUT\n",
    "            ) for _ in range(self.DEPTH)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.EMBEDDING)\n",
    "        self.head = nn.Linear(self.EMBEDDING, self.NUM_CLASS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        cls_out = self.norm(x[:, 0])\n",
    "        return self.head(cls_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1298444a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 3.31 MiB is free. Including non-PyTorch memory, this process has 3.67 GiB memory in use. Of the allocated memory 3.32 GiB is allocated by PyTorch, and 252.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(subset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ========== Model ==========\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionTransformerSmall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodelConfigDict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ========== Loss & Optimizer ==========\u001b[39;00m\n\u001b[1;32m     62\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.69 GiB of which 3.31 MiB is free. Including non-PyTorch memory, this process has 3.67 GiB memory in use. Of the allocated memory 3.32 GiB is allocated by PyTorch, and 252.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "# ========== Config ==========\n",
    "DATA_DIR = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/tiny-imagenet-200\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16  # for 100 samples\n",
    "LR = 3e-4\n",
    "\n",
    "modelConfigDict = {\n",
    "    'CHANNEL' : 3,\n",
    "    'PATCH' : 16,\n",
    "    'EMBEDDING' : 192,\n",
    "    'IMAGE' : 224,\n",
    "    'NUM_HEADS' : 3,\n",
    "    'MLP_RATIO' : 4.0,\n",
    "    'DROPOUT' : 0.0,\n",
    "    'NUM_CLASSES' : 20,\n",
    "    'DEPTH' : 12\n",
    "}    \n",
    "\n",
    "\n",
    "# ========== Transforms ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ========== Load Dataset ==========\n",
    "train_data = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "\n",
    "# Build class-balanced 100-sample subset: 20 classes  5 samples\n",
    "class_to_idx = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(train_data):\n",
    "    class_to_idx[label].append(idx)\n",
    "\n",
    "subset_indices = []\n",
    "for label in sorted(class_to_idx.keys())[:20]:  # 20 classes\n",
    "    subset_indices.extend(class_to_idx[label][:5])  # 5 samples each\n",
    "\n",
    "subset = Subset(train_data, subset_indices)\n",
    "loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# ========== Model ==========\n",
    "model = VisionTransformerSmall(**modelConfigDict).to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)  # No weight decay\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e009fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba794f0b",
   "metadata": {},
   "source": [
    "### Creating another version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c1646",
   "metadata": {},
   "source": [
    "#### V01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e82fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, embed_dim, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )  # [B, C, H, W] -> [B, embed_dim, H//P, W//P]\n",
    " \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)  # -> [B, embed_dim, H//P, W//P]\n",
    "        x = x.flatten(2).transpose(1, 2)  # -> [B, num_patches, embed_dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        assert dim % num_heads == 0, \"Embedding dim must be divisible by number of heads\"\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5  # Scaling factor: 1/sqrt(dk)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Fused Q, K, V\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)  # Final projection\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # Step 1: Linear layer to get Q, K, V\n",
    "        qkv = self.qkv(x)                      # [B, N, 3*C]\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)       # [3, B, num_heads, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]       # [B, num_heads, N, head_dim]\n",
    "\n",
    "        # Step 2: Compute scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, heads, N, N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # Step 3: Weighted sum of values\n",
    "        out = (attn @ v)  # [B, heads, N, head_dim]\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)  # [B, N, C]\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87403a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth (DropPath) as per https://arxiv.org/abs/1603.09382\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def drop_path_method(self, x, drop_prob: float = 0., training: bool = False):\n",
    "        if drop_prob == 0. or not training:\n",
    "            return x\n",
    "        keep_prob = 1 - drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # (B, 1, 1, ...)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop_path_method(x, self.drop_prob, self.training)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5eb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout, drop_path_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = Attention(dim=emb_size, num_heads=num_heads)\n",
    "        self.drop_path1 = DropPath(drop_path_rate)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.drop_path2 = DropPath(drop_path_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.attn(self.ln1(x)))  # Apply drop-path to attention output\n",
    "        x = x + self.drop_path2(self.mlp(self.ln2(x)))   # Apply drop-path to MLP output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c1789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block (basic)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(emb_size, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Mini ViT\n",
    "class MiniViT(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size=4, in_channels=3, num_classes=200,\n",
    "                 emb_size=384, depth=4, num_heads=6, mlp_ratio=3.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, emb_size))\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            TransformerBlock(emb_size, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.head = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x[:, 0])\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class VisionTransformerTiny(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_channels=3,\n",
    "                 num_classes=20,\n",
    "                 embed_dim=192,\n",
    "                 depth=12,\n",
    "                 num_heads=3,\n",
    "                 mlp_ratio=4.0,\n",
    "                 qkv_bias=True,\n",
    "                 drop_rate=0.0,\n",
    "                 attn_drop_rate=0.0,\n",
    "                 drop_path_rate=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Class token and positional embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Transformer blocks with gradually increasing drop_path\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=drop_rate,\n",
    "                drop_path_rate=dpr[i]\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, N, C)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, C)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, C)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]  # CLS token output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1be73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 6.00%\n",
      "Epoch 02 | Train Acc: 14.00%\n",
      "Epoch 03 | Train Acc: 20.00%\n",
      "Epoch 04 | Train Acc: 19.00%\n",
      "Epoch 05 | Train Acc: 26.00%\n",
      "Epoch 06 | Train Acc: 29.00%\n",
      "Epoch 07 | Train Acc: 30.00%\n",
      "Epoch 08 | Train Acc: 32.00%\n",
      "Epoch 09 | Train Acc: 40.00%\n",
      "Epoch 10 | Train Acc: 43.00%\n",
      "Epoch 11 | Train Acc: 45.00%\n",
      "Epoch 12 | Train Acc: 49.00%\n",
      "Epoch 13 | Train Acc: 47.00%\n",
      "Epoch 14 | Train Acc: 51.00%\n",
      "Epoch 15 | Train Acc: 55.00%\n",
      "Epoch 16 | Train Acc: 62.00%\n",
      "Epoch 17 | Train Acc: 66.00%\n",
      "Epoch 18 | Train Acc: 69.00%\n",
      "Epoch 19 | Train Acc: 69.00%\n",
      "Epoch 20 | Train Acc: 74.00%\n",
      "Epoch 21 | Train Acc: 76.00%\n",
      "Epoch 22 | Train Acc: 80.00%\n",
      "Epoch 23 | Train Acc: 89.00%\n",
      "Epoch 24 | Train Acc: 92.00%\n",
      "Epoch 25 | Train Acc: 95.00%\n",
      "Epoch 26 | Train Acc: 98.00%\n",
      "Epoch 27 | Train Acc: 97.00%\n",
      "Epoch 28 | Train Acc: 100.00%\n",
      "Model has successfully overfitted the subset.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "# ========== Config ==========\n",
    "DATA_DIR = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/tiny-imagenet-200\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16  # for 100 samples\n",
    "LR = 3e-4\n",
    "\n",
    "modelConfigDict = {\n",
    "    'CHANNEL' : 3,\n",
    "    'PATCH' : 16,\n",
    "    'EMBEDDING' : 192,\n",
    "    'IMAGE' : 224,\n",
    "    'NUM_HEADS' : 3,\n",
    "    'MLP_RATIO' : 4.0,\n",
    "    'DROPOUT' : 0.0,\n",
    "    'NUM_CLASSES' : 20,\n",
    "    'DEPTH' : 12\n",
    "}    \n",
    "\n",
    "\n",
    "# ========== Transforms ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ========== Load Dataset ==========\n",
    "train_data = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "\n",
    "# Build class-balanced 100-sample subset: 20 classes  5 samples\n",
    "class_to_idx = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(train_data):\n",
    "    class_to_idx[label].append(idx)\n",
    "\n",
    "subset_indices = []\n",
    "for label in sorted(class_to_idx.keys())[:20]:  # 20 classes\n",
    "    subset_indices.extend(class_to_idx[label][:5])  # 5 samples each\n",
    "\n",
    "subset = Subset(train_data, subset_indices)\n",
    "loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# ========== Model ==========\n",
    "#model = VisionTransformerSmall(**modelConfigDict).to(DEVICE)\n",
    "model = VisionTransformerTiny().to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)  # No weight decay\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50e49e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ROOT_DIR_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mROOT_DIR_PATH\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ROOT_DIR_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "ROOT_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f44387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset : TINYIMAGENET200\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/ already exists, zip downloaded.\n",
      "TinyImageNet already exists under: /home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/\n",
      "training size  : 100\n",
      "validation size : 10000\n",
      "Subset contains 20 unique classes\n",
      "Sample label: 0\n",
      "Train batches: 7, Validation batches: 625\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([16, 3, 224, 224]), torch.Size([16])\n",
      "Epoch 01 | Train Acc: 3.00%\n",
      "Epoch 02 | Train Acc: 13.00%\n",
      "Epoch 03 | Train Acc: 15.00%\n",
      "Epoch 04 | Train Acc: 28.00%\n",
      "Epoch 05 | Train Acc: 23.00%\n",
      "Epoch 06 | Train Acc: 34.00%\n",
      "Epoch 07 | Train Acc: 29.00%\n",
      "Epoch 08 | Train Acc: 40.00%\n",
      "Epoch 09 | Train Acc: 36.00%\n",
      "Epoch 10 | Train Acc: 32.00%\n",
      "Epoch 11 | Train Acc: 34.00%\n",
      "Epoch 12 | Train Acc: 36.00%\n",
      "Epoch 13 | Train Acc: 44.00%\n",
      "Epoch 14 | Train Acc: 45.00%\n",
      "Epoch 15 | Train Acc: 41.00%\n",
      "Epoch 16 | Train Acc: 61.00%\n",
      "Epoch 17 | Train Acc: 58.00%\n",
      "Epoch 18 | Train Acc: 71.00%\n",
      "Epoch 19 | Train Acc: 69.00%\n",
      "Epoch 20 | Train Acc: 75.00%\n",
      "Epoch 21 | Train Acc: 76.00%\n",
      "Epoch 22 | Train Acc: 76.00%\n",
      "Epoch 23 | Train Acc: 84.00%\n",
      "Epoch 24 | Train Acc: 89.00%\n",
      "Epoch 25 | Train Acc: 91.00%\n",
      "Epoch 26 | Train Acc: 90.00%\n",
      "Epoch 27 | Train Acc: 95.00%\n",
      "Epoch 28 | Train Acc: 98.00%\n",
      "Epoch 29 | Train Acc: 97.00%\n",
      "Epoch 30 | Train Acc: 96.00%\n",
      "Epoch 31 | Train Acc: 98.00%\n",
      "Epoch 32 | Train Acc: 95.00%\n",
      "Epoch 33 | Train Acc: 97.00%\n",
      "Epoch 34 | Train Acc: 95.00%\n",
      "Epoch 35 | Train Acc: 96.00%\n",
      "Epoch 36 | Train Acc: 100.00%\n",
      "Model has successfully overfitted the subset.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "# ========== Config ==========\n",
    "DATA_DIR = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/TINYIMAGENET200/tiny-imagenet-200\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16  # for 100 samples\n",
    "LR = 3e-4\n",
    "\n",
    "# # ========== Transforms ==========\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                          [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # ========== Load Dataset ==========\n",
    "# train_data = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils.data_loader import DatasetLoader\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path\n",
    "\n",
    "from utils.config_loader import load_config\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "trainingConfig = config['training_dummy']\n",
    "DATASET = 'TINYIMAGENET200'\n",
    "DATA_DIR = f'{ROOT_DIR_PATH}/data/{DATASET}/'\n",
    "# loading data\n",
    "print(f'loading dataset : {DATASET}')\n",
    "loader = DatasetLoader(training_config=trainingConfig,\n",
    "                        dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=16,\n",
    "                        num_workers=4,\n",
    "                        img_size=224)\n",
    "train_loader, val_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break\n",
    "\n",
    "\n",
    "# ========== Model ==========\n",
    "#model = VisionTransformerSmall(**modelConfigDict).to(DEVICE)\n",
    "\n",
    "from model.vit_custom import VisionTransformerTiny\n",
    "model = VisionTransformerTiny(\n",
    "                img_size = 224,\n",
    "                 patch_size = 16,\n",
    "                 in_channels = 3,\n",
    "                 num_classes = 20,\n",
    "                 embed_dim = 192,\n",
    "                 depth = 12,\n",
    "                 num_heads=3,\n",
    "                 mlp_ratio=4.0,\n",
    "                 qkv_bias=True,\n",
    "                 dropout=0.0,\n",
    "                 attn_drop_rate = 0.0,\n",
    "                 drop_path_rate = 0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)  # No weight decay\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928fe15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
