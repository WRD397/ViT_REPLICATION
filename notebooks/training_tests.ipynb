{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c0ef90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11a34f",
   "metadata": {},
   "source": [
    "# Model Architecture and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f67d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# config numbers\n",
    "class ViTConfig:\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 in_channels=3,\n",
    "                 emb_size=64,\n",
    "                 depth=6,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.1):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_size = emb_size\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)        # (B, emb_size, H/patch, W/patch)\n",
    "        x = x.flatten(2)              # (B, emb_size, N)\n",
    "        x = x.transpose(1, 2)         # (B, N, emb_size)\n",
    "        return x                      # Shape: (B, N, E)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Vit Test Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        cfg = config[\"model\"]\n",
    "        \n",
    "        self.CHANNEL = cfg[\"in_channels\"]\n",
    "        self.PATCH = cfg[\"patch_size\"]\n",
    "        self.EMBEDDING = cfg[\"emb_size\"]\n",
    "        self.IMAGE = cfg[\"img_size\"]\n",
    "        self.NUM_HEADS = cfg[\"num_heads\"]\n",
    "        self.MLP_RATIO = cfg[\"mlp_ratio\"]\n",
    "        self.DROPOUT = cfg[\"dropout\"]\n",
    "        self.NUM_CLASS = cfg[\"num_classes\"]\n",
    "        self.DEPTH = cfg[\"depth\"]\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=self.CHANNEL,\n",
    "            patch_size=self.PATCH,\n",
    "            emb_size=self.EMBEDDING,\n",
    "            img_size=self.IMAGE\n",
    "        )\n",
    "\n",
    "        self.n_patches = (self.IMAGE// self.PATCH) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.EMBEDDING))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, self.EMBEDDING))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=self.EMBEDDING,\n",
    "                num_heads=self.NUM_HEADS,\n",
    "                mlp_ratio=self.MLP_RATIO,\n",
    "                dropout=self.DROPOUT\n",
    "            ) for _ in range(self.DEPTH)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.EMBEDDING)\n",
    "        self.head = nn.Linear(self.EMBEDDING, self.NUM_CLASS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        cls_out = self.norm(x[:, 0])\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8728691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "\n",
    "# loading config file for model\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_test_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85f433e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'dataset': 'CIFAR10',\n",
       "  'data_path': '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10',\n",
       "  'batch_size': 32,\n",
       "  'num_workers': 4,\n",
       "  'img_size': 32},\n",
       " 'model': {'img_size': 32,\n",
       "  'patch_size': 4,\n",
       "  'in_channels': 3,\n",
       "  'emb_size': 128,\n",
       "  'depth': 4,\n",
       "  'num_heads': 4,\n",
       "  'mlp_ratio': 4.0,\n",
       "  'num_classes': 10,\n",
       "  'dropout': 0.1},\n",
       " 'train': {'batch_size': 32, 'epochs': 20, 'lr': 0.001}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8828f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model sanity\n",
      "torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 10])\n",
      "loading training testing data\n",
      "Train batches: 1563, Test batches: 313\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([32, 3, 32, 32]), torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(config)\n",
    "\n",
    "print('testing model sanity')\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "print(dummy_input.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([1, 10])\n",
    "\n",
    "\n",
    "print('loading training testing data')\n",
    "# loading config file for CIFAR10\n",
    "data_cfg = config[\"data\"]\n",
    "DATASET = data_cfg[\"dataset\"]\n",
    "DATA_DIR = data_cfg[\"data_path\"]\n",
    "BATCH = data_cfg[\"batch_size\"]\n",
    "NUM_WORKERS = data_cfg[\"num_workers\"]\n",
    "IMAGE = data_cfg[\"img_size\"]\n",
    "\n",
    "# loading data\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, test_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb150eed",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f2a7b",
   "metadata": {},
   "source": [
    "### running forward pass for one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7542a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5451, -0.5216, -0.3725,  ..., -0.0745, -0.1373, -0.2941],\n",
       "          [-0.5765, -0.5529, -0.4902,  ..., -0.1294, -0.2392, -0.2706],\n",
       "          [-0.5529, -0.5608, -0.6078,  ..., -0.1608, -0.2157, -0.1686],\n",
       "          ...,\n",
       "          [ 0.8431,  0.8667,  0.8431,  ...,  0.8588,  0.8275,  0.7725],\n",
       "          [ 0.8275,  0.8353,  0.8431,  ...,  0.7412,  0.7333,  0.7412],\n",
       "          [ 0.7725,  0.7804,  0.7961,  ...,  0.7647,  0.7490,  0.7098]],\n",
       "\n",
       "         [[-0.3098, -0.2941, -0.1294,  ...,  0.0902,  0.0510, -0.0667],\n",
       "          [-0.3490, -0.3098, -0.2863,  ...,  0.0667, -0.0275, -0.0510],\n",
       "          [-0.3333, -0.3098, -0.4353,  ...,  0.0667,  0.0118,  0.0510],\n",
       "          ...,\n",
       "          [ 0.6078,  0.6078,  0.6471,  ...,  0.6314,  0.6078,  0.5529],\n",
       "          [ 0.5765,  0.5686,  0.6314,  ...,  0.5059,  0.5059,  0.5216],\n",
       "          [ 0.5216,  0.5137,  0.5765,  ...,  0.5608,  0.5608,  0.5216]],\n",
       "\n",
       "         [[-0.5843, -0.5686, -0.4118,  ..., -0.2784, -0.3569, -0.5137],\n",
       "          [-0.6000, -0.5608, -0.5294,  ..., -0.3725, -0.5059, -0.5529],\n",
       "          [-0.5608, -0.5451, -0.6392,  ..., -0.4039, -0.4902, -0.4667],\n",
       "          ...,\n",
       "          [ 0.4275,  0.4431,  0.4588,  ...,  0.4275,  0.3412,  0.2784],\n",
       "          [ 0.3647,  0.3725,  0.4118,  ...,  0.2784,  0.2157,  0.2235],\n",
       "          [ 0.2471,  0.2471,  0.2941,  ...,  0.3412,  0.2784,  0.2314]]],\n",
       "\n",
       "\n",
       "        [[[-0.2235, -0.3020, -0.2941,  ..., -0.4824, -0.4431, -0.4353],\n",
       "          [-0.3098, -0.3490, -0.3020,  ..., -0.5216, -0.4588, -0.4431],\n",
       "          [-0.2863, -0.3098, -0.3020,  ..., -0.5451, -0.4824, -0.4353],\n",
       "          ...,\n",
       "          [ 0.6941,  0.5922,  0.6863,  ...,  0.2627,  0.3333,  0.4902],\n",
       "          [ 0.6078,  0.5294,  0.7098,  ...,  0.5294,  0.4745,  0.5373],\n",
       "          [ 0.7020,  0.6549,  0.7020,  ...,  0.6157,  0.4745,  0.5608]],\n",
       "\n",
       "         [[-0.2863, -0.3490, -0.3412,  ..., -0.5294, -0.4824, -0.4745],\n",
       "          [-0.3569, -0.3961, -0.3412,  ..., -0.5765, -0.4980, -0.4824],\n",
       "          [-0.2941, -0.3333, -0.3020,  ..., -0.6000, -0.5216, -0.4745],\n",
       "          ...,\n",
       "          [ 0.7647,  0.6941,  0.7882,  ...,  0.3020,  0.3647,  0.5608],\n",
       "          [ 0.6941,  0.6471,  0.8118,  ...,  0.6157,  0.5686,  0.6392],\n",
       "          [ 0.7882,  0.7490,  0.7647,  ...,  0.7176,  0.5922,  0.6784]],\n",
       "\n",
       "         [[-0.5294, -0.5686, -0.5451,  ..., -0.6706, -0.6314, -0.6235],\n",
       "          [-0.5529, -0.5843, -0.5608,  ..., -0.7020, -0.6627, -0.6392],\n",
       "          [-0.4196, -0.4667, -0.4902,  ..., -0.7333, -0.6941, -0.6471],\n",
       "          ...,\n",
       "          [ 0.7412,  0.6471,  0.7412,  ...,  0.2706,  0.3098,  0.4902],\n",
       "          [ 0.6627,  0.5843,  0.7490,  ...,  0.5843,  0.5216,  0.5922],\n",
       "          [ 0.7490,  0.6941,  0.7020,  ...,  0.6784,  0.5686,  0.6471]]],\n",
       "\n",
       "\n",
       "        [[[-0.8510, -0.8510, -0.8275,  ..., -0.8431, -0.8196, -0.8196],\n",
       "          [-0.8353, -0.8353, -0.8275,  ..., -0.8118, -0.8196, -0.8353],\n",
       "          [-0.8275, -0.8431, -0.8275,  ..., -0.8118, -0.8275, -0.8353],\n",
       "          ...,\n",
       "          [-0.8667, -0.8510, -0.8510,  ..., -0.4275, -0.7412, -0.8588],\n",
       "          [-0.8588, -0.8824, -0.9059,  ..., -0.0902, -0.2078, -0.5059],\n",
       "          [-0.9059, -0.9137, -0.9059,  ...,  0.2471,  0.2392, -0.0353]],\n",
       "\n",
       "         [[-0.7098, -0.7098, -0.6863,  ..., -0.6235, -0.6000, -0.6000],\n",
       "          [-0.6941, -0.6941, -0.6863,  ..., -0.5922, -0.6000, -0.6157],\n",
       "          [-0.6863, -0.7020, -0.6863,  ..., -0.5922, -0.6078, -0.6157],\n",
       "          ...,\n",
       "          [-0.7725, -0.7569, -0.7569,  ..., -0.2392, -0.5529, -0.7020],\n",
       "          [-0.7647, -0.7882, -0.8118,  ...,  0.0824, -0.0431, -0.3490],\n",
       "          [-0.8118, -0.8196, -0.8118,  ...,  0.4118,  0.4118,  0.1294]],\n",
       "\n",
       "         [[-0.6000, -0.6000, -0.5765,  ..., -0.5294, -0.5059, -0.5059],\n",
       "          [-0.5843, -0.5843, -0.5765,  ..., -0.4980, -0.5059, -0.5216],\n",
       "          [-0.5686, -0.5922, -0.5765,  ..., -0.4980, -0.5137, -0.5216],\n",
       "          ...,\n",
       "          [-0.6784, -0.6627, -0.6627,  ..., -0.2157, -0.4745, -0.6235],\n",
       "          [-0.6706, -0.6941, -0.7176,  ...,  0.0510, -0.0431, -0.3333],\n",
       "          [-0.7098, -0.7255, -0.7176,  ...,  0.3176,  0.3412,  0.0824]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.6549,  0.4667,  0.4745,  ...,  0.3961,  0.3412,  0.3569],\n",
       "          [ 0.5059,  0.5608,  0.6000,  ...,  0.1686,  0.1686,  0.2863],\n",
       "          [ 0.3569,  0.5765,  0.6235,  ..., -0.0431, -0.0196,  0.1765],\n",
       "          ...,\n",
       "          [-0.2941, -0.3098, -0.5529,  ...,  0.2863,  0.0510, -0.0980],\n",
       "          [-0.0588,  0.0039, -0.6000,  ...,  0.4588,  0.1765, -0.0275],\n",
       "          [-0.0980,  0.2235, -0.1922,  ...,  0.2941,  0.1373, -0.0510]],\n",
       "\n",
       "         [[ 0.4824,  0.2941,  0.3020,  ...,  0.2392,  0.1843,  0.2000],\n",
       "          [ 0.3333,  0.3882,  0.4275,  ...,  0.0118,  0.0118,  0.1294],\n",
       "          [ 0.1843,  0.4039,  0.4510,  ..., -0.2000, -0.1765,  0.0196],\n",
       "          ...,\n",
       "          [-0.4039, -0.4431, -0.6863,  ...,  0.1922, -0.0431, -0.1922],\n",
       "          [-0.1686, -0.1294, -0.7333,  ...,  0.3647,  0.0824, -0.1216],\n",
       "          [-0.2157,  0.0902, -0.3255,  ...,  0.2000,  0.0431, -0.1451]],\n",
       "\n",
       "         [[ 0.5843,  0.3961,  0.4039,  ...,  0.3098,  0.2549,  0.2706],\n",
       "          [ 0.4353,  0.4902,  0.5294,  ...,  0.0824,  0.0824,  0.2000],\n",
       "          [ 0.2863,  0.5059,  0.5529,  ..., -0.1294, -0.1059,  0.0902],\n",
       "          ...,\n",
       "          [-0.2471, -0.2941, -0.6235,  ...,  0.3020,  0.0667, -0.0824],\n",
       "          [ 0.0118,  0.0196, -0.6549,  ...,  0.4745,  0.1922, -0.0118],\n",
       "          [-0.0196,  0.2471, -0.2392,  ...,  0.3098,  0.1529, -0.0353]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0353,  0.0431,  0.1059,  ...,  0.1922,  0.1451,  0.1059],\n",
       "          [-0.0588, -0.0275,  0.0745,  ...,  0.1765,  0.1294,  0.0431],\n",
       "          [-0.0431, -0.0039,  0.0980,  ...,  0.1765,  0.1294,  0.0431],\n",
       "          ...,\n",
       "          [-0.7333, -0.6627, -0.6000,  ..., -0.4745, -0.5373, -0.5686],\n",
       "          [-0.7804, -0.7176, -0.6549,  ..., -0.5216, -0.5608, -0.5922],\n",
       "          [-0.4275, -0.3961, -0.3725,  ..., -0.3098, -0.3412, -0.3412]],\n",
       "\n",
       "         [[ 0.0667,  0.1529,  0.2235,  ...,  0.3569,  0.2863,  0.2078],\n",
       "          [ 0.0353,  0.1373,  0.2471,  ...,  0.4039,  0.3333,  0.1922],\n",
       "          [ 0.0980,  0.2157,  0.3255,  ...,  0.4431,  0.3725,  0.2392],\n",
       "          ...,\n",
       "          [-0.7647, -0.6941, -0.6314,  ..., -0.5059, -0.5686, -0.5922],\n",
       "          [-0.8118, -0.7490, -0.6863,  ..., -0.5529, -0.5922, -0.6157],\n",
       "          [-0.4588, -0.4275, -0.4039,  ..., -0.3412, -0.3725, -0.3725]],\n",
       "\n",
       "         [[ 0.1608,  0.2784,  0.3255,  ...,  0.3804,  0.3255,  0.2471],\n",
       "          [ 0.1608,  0.3020,  0.3882,  ...,  0.4667,  0.4039,  0.2627],\n",
       "          [ 0.2471,  0.3961,  0.4824,  ...,  0.5294,  0.4667,  0.3176],\n",
       "          ...,\n",
       "          [-0.7569, -0.6863, -0.6235,  ..., -0.4980, -0.5608, -0.5843],\n",
       "          [-0.8039, -0.7412, -0.6784,  ..., -0.5451, -0.5843, -0.6078],\n",
       "          [-0.4510, -0.4196, -0.3961,  ..., -0.3333, -0.3647, -0.3647]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000,  0.9686,  0.9765,  ...,  0.9765,  0.9765,  0.9765],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9922,  0.9922,  0.9922],\n",
       "          [ 1.0000,  0.9843,  0.9843,  ...,  0.9765,  0.9765,  0.9608],\n",
       "          ...,\n",
       "          [ 0.9686,  0.5451, -0.0118,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 1.0000,  0.9294,  0.7333,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.9922,  0.9608,  0.9451,  ...,  0.9765,  0.9765,  0.9765]],\n",
       "\n",
       "         [[ 1.0000,  0.9686,  0.9765,  ...,  0.9686,  0.9686,  0.9686],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  0.9843,  0.9843,  ...,  0.9765,  0.9843,  0.9843],\n",
       "          ...,\n",
       "          [ 0.9765,  0.5529, -0.0039,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 1.0000,  0.9294,  0.7333,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.9922,  0.9608,  0.9451,  ...,  0.9765,  0.9765,  0.9765]],\n",
       "\n",
       "         [[ 1.0000,  0.9686,  0.9765,  ...,  0.9765,  0.9843,  0.9843],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  0.9843,  0.9843,  ...,  0.9765,  0.9843,  0.9922],\n",
       "          ...,\n",
       "          [ 0.9765,  0.5529, -0.0039,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 1.0000,  0.9294,  0.7333,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.9922,  0.9608,  0.9451,  ...,  0.9765,  0.9765,  0.9765]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This creates an iterator from an iterable object.\n",
    "#Fetches the next element from the iterator.\n",
    "\n",
    "# 1st batch of 32 images\n",
    "print(next(iter(train_loader))[0].shape)\n",
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d232d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fdda0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f08458",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8d4322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe820a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f6b359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._MultiProcessingDataLoaderIter at 0x7904f40c9b40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get one batch\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daa40a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44912caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdb637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4418d6",
   "metadata": {},
   "source": [
    "#### 2. Send to device\n",
    "\n",
    "PyTorch supports running tensors either on:\n",
    "\n",
    "CPU → default if you're not using a GPU\n",
    "\n",
    "GPU (CUDA) → much faster for deep learning\n",
    "\n",
    "#### images.to(device) will -\n",
    "➡️ \"Move this tensor (images) to the computing device (device) you specify — either CPU or GPU.\"\n",
    "\n",
    "#### Hardware level\n",
    "\n",
    "💻 1. Your CPU and GPU Have Separate Memory\n",
    "Component\tType of Memory\tExample Usage\n",
    "CPU\tRAM (main memory)\timages tensor when first loaded\n",
    "GPU\tVRAM (device memory)\timages.to(\"cuda\") sends it here\n",
    "\n",
    "| Component | Type of Memory       | Example Usage                     |\n",
    "| --------- | -------------------- | --------------------------------- |\n",
    "| CPU       | RAM (main memory)    | `images` tensor when first loaded |\n",
    "| GPU       | VRAM (device memory) | `images.to(\"cuda\")` sends it here |\n",
    "\n",
    "\n",
    "The CPU and GPU do not share memory. So if you want to use the GPU to compute something, you must copy the tensor from CPU RAM to GPU VRAM.\n",
    "\n",
    "⚙️ 2. What Happens Internally?\n",
    "Here’s the process when you run tensor.to(\"cuda\"):\n",
    "\n",
    "PyTorch checks which device the tensor is currently on.\n",
    "\n",
    "Allocates memory in the GPU VRAM to hold the tensor.\n",
    "\n",
    "Copies data from system RAM to GPU memory using the CUDA driver.\n",
    "\n",
    "Returns a new tensor that lives on the GPU.\n",
    "\n",
    "This data transfer uses the PCIe (Peripheral Component Interconnect Express) bus — this is the \"highway\" that connects your CPU and GPU physically.\n",
    "\n",
    "\n",
    "Slow: Moving data over PCIe is relatively slow. So if you do .to(\"cuda\") inside your training loop (on every batch), it will kill your performance.\n",
    "\n",
    "Plan ahead: Always move both your model and data to the GPU before training or inference begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7223da7",
   "metadata": {},
   "source": [
    "#### `Run it before and after running .to(\"cuda\") — you’ll see the memory usage spike as your tensor lands in VRAM.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bee0b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e314cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images, labels = images.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1329f",
   "metadata": {},
   "source": [
    "#### 3. Load model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961a7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faf12d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.VisionTransformer"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58140cae",
   "metadata": {},
   "source": [
    "#### 4. Set model to eval mode (we just want to inspect)\n",
    "\n",
    "✅ Layers Affected\n",
    "Dropout (nn.Dropout)\n",
    "\n",
    "In train() mode: randomly zeroes out some neurons with a probability p (adds noise for regularization).\n",
    "\n",
    "In eval() mode: no neurons are dropped — it just passes the values through.\n",
    "\n",
    "BatchNorm (nn.BatchNorm1d/2d/3d)\n",
    "\n",
    "In train() mode: uses batch statistics (mean and variance) and updates running stats.\n",
    "\n",
    "In eval() mode: uses the running averages (learned from training), and does not update them.\n",
    "\n",
    "\n",
    "✅ What model.eval() does not do\n",
    "\n",
    "It does not:\n",
    "\n",
    "Disable gradient computation (you need torch.no_grad() for that).\n",
    "\n",
    "Freeze model weights.\n",
    "\n",
    "Turn off autograd.\n",
    "\n",
    "Stop the model from being trained if optimizer.step() is still called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a2a05e",
   "metadata": {},
   "source": [
    "#### 5. Forward pass with torch.no_grad\n",
    "\n",
    "#### `torch.no_grad`\n",
    "\n",
    "❗ If you're only doing a forward pass (no loss.backward() and no optimizer.step()), the model parameters will not get updated — with or without torch.no_grad().\n",
    "\n",
    "#### Purpose of torch.no_grad() (even during forward-only pass):\n",
    "\n",
    "Saves Memory:\n",
    "\n",
    "PyTorch usually tracks every operation on tensors to later compute gradients.\n",
    "\n",
    "If you're not planning to backpropagate (e.g. during evaluation or debugging), this is unnecessary overhead.\n",
    "\n",
    "torch.no_grad() disables autograd, making things faster and leaner.\n",
    "\n",
    "Speeds Up Inference:\n",
    "\n",
    "Especially helpful in evaluation, validation, and visualizations.\n",
    "\n",
    "No need to maintain a compute graph → performance is better.\n",
    "\n",
    "Prevents Accidental Training:\n",
    "\n",
    "Sometimes, we accidentally call .backward() or .step() inside inference loops.\n",
    "\n",
    "Wrapping your code in torch.no_grad() makes it safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af32e5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe1eaaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Vectors: torch.Size([32, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    patch_vectors = model.patch_embed(images)\n",
    "    print(\"Patch Vectors:\", patch_vectors.shape)  # (B, N, E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad47a10",
   "metadata": {},
   "source": [
    "batchsize is 32, patch size is 4, so 32 X 32 image becomes (32/4)*(32/4) = 64 number of patches. 128 embedding dimensions for each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Vectors: torch.Size([32, 64, 128])\n",
      "After Adding CLS Token: torch.Size([32, 65, 128])\n",
      "After Adding Positional Embedding: torch.Size([32, 65, 128])\n",
      "After Encoder Block 1: torch.Size([32, 65, 128])\n",
      "After Encoder Block 2: torch.Size([32, 65, 128])\n",
      "After Encoder Block 3: torch.Size([32, 65, 128])\n",
      "After Encoder Block 4: torch.Size([32, 65, 128])\n",
      "🔹 Final CLS Token Output: torch.Size([32, 128])\n",
      "🔹 Logits Output: torch.Size([32, 10])\n",
      "🔹 Predicted Classes: tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 0, 6, 0, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    patch_vectors = model.patch_embed(images)\n",
    "    print(\"Patch Vectors:\", patch_vectors.shape)  # (B, N, E)\n",
    "\n",
    "    ## getting the batch size to match the cls_toke dimension so that we can add it with the patch embedding vector    \n",
    "    B = images.shape[0]\n",
    "    cls_token = model.cls_token.expand(B, -1, -1)\n",
    "    \n",
    "    #Concatenate the [CLS] token at the start of patch tokens along the sequence dimension (dim=1).\n",
    "    #Resulting shape: (B, N+1, E) — one extra token.\n",
    "    patch_with_cls = torch.cat((cls_token, patch_vectors), dim=1)\n",
    "    print(\"After Adding CLS Token:\", patch_with_cls.shape)  # (B, N+1, E)\n",
    "\n",
    "    x = patch_with_cls + model.pos_embedding[:, :patch_with_cls.size(1), :]\n",
    "    print(\"After Adding Positional Embedding:\", x.shape)\n",
    "\n",
    "    # Pass through encoder\n",
    "    for i, block in enumerate(model.encoder):\n",
    "        x = block(x)\n",
    "        print(f\"After Encoder Block {i+1}: {x.shape}\")\n",
    "\n",
    "    # Final CLS token output\n",
    "    cls_output = model.norm(x[:, 0])\n",
    "    print(\"🔹 Final CLS Token Output:\", cls_output.shape)\n",
    "\n",
    "    # Final prediction\n",
    "    output = model.head(cls_output)\n",
    "    print(\"🔹 Logits Output:\", output.shape)\n",
    "    print(\"🔹 Predicted Classes:\", output.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c99847db",
   "metadata": {},
   "source": [
    "## Running One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae244a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Avoid division by zero on first step\n",
    "            if total > 0:\n",
    "                avg_loss = running_loss / total\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                    \"Acc\": f\"{accuracy:.2f}%\"\n",
    "                })\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for  inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Update progress bar with metrics\n",
    "        if total > 0:\n",
    "            avg_loss = running_loss / total\n",
    "            accuracy = 100. * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                \"Loss\": f\"{avg_loss:.4f}\",\n",
    "                \"Acc\": f\"{accuracy:.2f}%\"\n",
    "            })\n",
    "\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8606872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = VisionTransformer(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6fbc178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf96b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training testing data\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "print('loading training testing data')\n",
    "# loading config file for CIFAR10\n",
    "data_cfg = config[\"data\"]\n",
    "DATASET = data_cfg[\"dataset\"]\n",
    "DATA_DIR = data_cfg[\"data_path\"]\n",
    "BATCH = data_cfg[\"batch_size\"]\n",
    "NUM_WORKERS = data_cfg[\"num_workers\"]\n",
    "IMAGE = data_cfg[\"img_size\"]\n",
    "\n",
    "# loading data\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, val_loader = loader.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19540d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7339, Accuracy: 36.90%\n",
      "Val   Loss: 1.5392, Accuracy: 44.58%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "#for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "\n",
    "#running for one epoch\n",
    "for epoch in range(1):\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['train']['epochs']}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# deleting GPU memory cache\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e14bd",
   "metadata": {},
   "source": [
    "## Few Important Observations\n",
    "\n",
    "1. Why does accuracy improve when rerunning training cells multiple times?\n",
    "This happens because your model is continuing training from where it left off in RAM/GPU memory — you're not reinitializing the model weights in between runs. So the model is learning incrementally across multiple cell runs, just like extra training epochs.\n",
    "\n",
    "Solution if you want fresh training each time:\n",
    "Reinitialize the model each time before running training, or restart the kernel to clear memory.\n",
    "\n",
    "2. Why does accuracy drop after restarting the kernel?\n",
    "When you restart the kernel, all variables including model weights are reset to their initial random state, so the model forgets everything it had learned in earlier training runs. That’s why accuracy drops back to what you'd expect from a randomly initialized model.\n",
    "\n",
    "This is expected behavior unless you load a saved model checkpoint.\n",
    "\n",
    "4. High CPU usage (98%) and persistent GPU memory usage (285 MiB)?\n",
    "\n",
    "- High CPU usage:\n",
    "This is from data loading, especially if you set num_workers > 0 in your DataLoader. PyTorch uses CPU subprocesses to load/transform data while GPU runs the model. 98% CPU is okay — it means your CPU is trying to keep up with GPU.\n",
    "\n",
    "- GPU memory stays occupied (even when not training):\n",
    "This is due to PyTorch caching.\n",
    "\n",
    "PyTorch doesn’t free GPU memory immediately after computations.\n",
    "Also, model weights and buffers stay on GPU unless you delete the model or restart the kernel.\n",
    "\n",
    "5. Why does GPU memory still show usage after del model and torch.cuda.empty_cache()?\n",
    "\n",
    "PyTorch’s caching allocator: PyTorch doesn’t release memory back to the GPU driver immediately after you delete tensors or call empty_cache(). Instead, it keeps the freed memory cached internally for future allocations to speed up subsequent GPU operations.\n",
    "\n",
    "Driver and system reserved memory: Some portion of GPU memory is always reserved by the GPU driver and the CUDA runtime itself, which shows up as occupied but cannot be freed.\n",
    "\n",
    "Other processes: Sometimes other system or user processes (like display server, background tasks, or other apps) use some GPU memory.\n",
    "\n",
    "6. Training:  70%|██████▉   | 1089/1563 [00:17<00:07, 64.15it/s]\n",
    "- Why 1563?\n",
    "\n",
    "It comes from the size of your training dataset and the batch size you set.\n",
    "\n",
    "For example:\n",
    "\n",
    "Suppose your training dataset size is 50,000 images (which is the case for CIFAR-10 training set).\n",
    "\n",
    "Your batch size is 32.\n",
    "\n",
    "- The number of batches per epoch (also called iterations per epoch) is:\n",
    "\n",
    "number of batches = dataset_size / batch_size = ceil(5000/32) = 1563\n",
    "\n",
    "- 1562 batches of 32 images each = 1562 * 32 = 49,984 images\n",
    "\n",
    "- The last batch will have the remaining 16 images to make up the total 50,000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dd361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93081416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
