{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0ef90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "CURR_PATH = f'/home/wd/Documents/work_stuff/ViT_REPLICATION'\n",
    "sys.path.append(os.path.abspath(CURR_PATH))  # Adds root directory to sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11a34f",
   "metadata": {},
   "source": [
    "# Model Architecture and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f67d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# config numbers\n",
    "class ViTConfig:\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 in_channels=3,\n",
    "                 emb_size=64,\n",
    "                 depth=6,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.1):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_size = emb_size\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)        # (B, emb_size, H/patch, W/patch)\n",
    "        x = x.flatten(2)              # (B, emb_size, N)\n",
    "        x = x.transpose(1, 2)         # (B, N, emb_size)\n",
    "        return x                      # Shape: (B, N, E)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Vit Test Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        cfg = config[\"model\"]\n",
    "        \n",
    "        self.CHANNEL = cfg[\"in_channels\"]\n",
    "        self.PATCH = cfg[\"patch_size\"]\n",
    "        self.EMBEDDING = cfg[\"emb_size\"]\n",
    "        self.IMAGE = cfg[\"img_size\"]\n",
    "        self.NUM_HEADS = cfg[\"num_heads\"]\n",
    "        self.MLP_RATIO = cfg[\"mlp_ratio\"]\n",
    "        self.DROPOUT = cfg[\"dropout\"]\n",
    "        self.NUM_CLASS = cfg[\"num_classes\"]\n",
    "        self.DEPTH = cfg[\"depth\"]\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=self.CHANNEL,\n",
    "            patch_size=self.PATCH,\n",
    "            emb_size=self.EMBEDDING,\n",
    "            img_size=self.IMAGE\n",
    "        )\n",
    "\n",
    "        self.n_patches = (self.IMAGE// self.PATCH) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.EMBEDDING))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, self.EMBEDDING))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=self.EMBEDDING,\n",
    "                num_heads=self.NUM_HEADS,\n",
    "                mlp_ratio=self.MLP_RATIO,\n",
    "                dropout=self.DROPOUT\n",
    "            ) for _ in range(self.DEPTH)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.EMBEDDING)\n",
    "        self.head = nn.Linear(self.EMBEDDING, self.NUM_CLASS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        cls_out = self.norm(x[:, 0])\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8828f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model sanity\n",
      "torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 10])\n",
      "loading training testing data\n",
      "Train batches: 1563, Test batches: 313\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([32, 3, 32, 32]), torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "\n",
    "# loading config file for model\n",
    "config = load_config(f\"{CURR_PATH}/config/vit_test_config.yaml\")\n",
    "model = VisionTransformer(config)\n",
    "\n",
    "print('testing model sanity')\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "print(dummy_input.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([1, 10])\n",
    "\n",
    "\n",
    "print('loading training testing data')\n",
    "# loading config file for CIFAR10\n",
    "data_cfg = config[\"data\"]\n",
    "DATASET = data_cfg[\"dataset\"]\n",
    "DATA_DIR = data_cfg[\"data_path\"]\n",
    "BATCH = data_cfg[\"batch_size\"]\n",
    "NUM_WORKERS = data_cfg[\"num_workers\"]\n",
    "IMAGE = data_cfg[\"img_size\"]\n",
    "\n",
    "# loading data\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, test_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb150eed",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f2a7b",
   "metadata": {},
   "source": [
    "### running forward pass for one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7542a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6235,  0.7255,  0.8118,  ...,  0.0667,  0.2235,  0.4431],\n",
       "          [ 0.4824,  0.5608,  0.6078,  ...,  0.2784,  0.1686,  0.3412],\n",
       "          [ 0.5686,  0.5451,  0.5922,  ...,  0.2157,  0.2314,  0.2706],\n",
       "          ...,\n",
       "          [-0.2235, -0.2235, -0.3412,  ...,  0.6706,  0.7020,  0.6078],\n",
       "          [-0.1608, -0.1922, -0.2784,  ...,  0.7176,  0.7412,  0.6863],\n",
       "          [-0.1216,  0.0588,  0.2000,  ...,  0.7412,  0.7725,  0.7333]],\n",
       "\n",
       "         [[ 0.5373,  0.6392,  0.7020,  ..., -0.1294,  0.0431,  0.2627],\n",
       "          [ 0.3647,  0.4510,  0.4902,  ...,  0.0510, -0.0510,  0.1294],\n",
       "          [ 0.4431,  0.4275,  0.4745,  ..., -0.0431, -0.0275,  0.0275],\n",
       "          ...,\n",
       "          [-0.2863, -0.2549, -0.3333,  ...,  0.4745,  0.5059,  0.4118],\n",
       "          [-0.2314, -0.2235, -0.2706,  ...,  0.5451,  0.5686,  0.5059],\n",
       "          [-0.1922,  0.0275,  0.2078,  ...,  0.5765,  0.6078,  0.5686]],\n",
       "\n",
       "         [[ 0.3255,  0.4275,  0.5137,  ..., -0.2863, -0.1216,  0.1059],\n",
       "          [ 0.1686,  0.2471,  0.3020,  ..., -0.0980, -0.2000, -0.0196],\n",
       "          [ 0.2471,  0.2235,  0.2863,  ..., -0.1686, -0.1529, -0.1059],\n",
       "          ...,\n",
       "          [-0.2863, -0.2549, -0.3176,  ...,  0.3176,  0.3490,  0.2549],\n",
       "          [-0.2235, -0.2235, -0.2549,  ...,  0.3804,  0.4039,  0.3412],\n",
       "          [-0.1843,  0.0353,  0.2235,  ...,  0.4118,  0.4431,  0.4039]]],\n",
       "\n",
       "\n",
       "        [[[-0.0902, -0.1529, -0.0980,  ...,  0.2392, -0.2392, -0.9608],\n",
       "          [-0.0588, -0.2314, -0.1137,  ...,  0.2078, -0.3490, -0.9216],\n",
       "          [-0.0510, -0.0745,  0.0588,  ...,  0.3725, -0.0824, -0.4980],\n",
       "          ...,\n",
       "          [ 0.4196,  0.6471,  0.7961,  ...,  0.7569,  0.8431,  0.8588],\n",
       "          [ 0.1529,  0.3412,  0.5529,  ...,  0.6314,  0.7255,  0.8118],\n",
       "          [-0.1765, -0.0196,  0.1373,  ...,  0.8039,  0.7882,  0.8196]],\n",
       "\n",
       "         [[-0.3098, -0.3804, -0.2941,  ..., -0.2314, -0.4824, -0.9608],\n",
       "          [-0.3020, -0.4510, -0.3098,  ..., -0.2706, -0.5843, -0.9373],\n",
       "          [-0.3490, -0.3255, -0.0980,  ..., -0.0118, -0.3882, -0.6314],\n",
       "          ...,\n",
       "          [ 0.2235,  0.4431,  0.6392,  ...,  0.5608,  0.6784,  0.6941],\n",
       "          [-0.0824,  0.1294,  0.3804,  ...,  0.3804,  0.4431,  0.5294],\n",
       "          [-0.3804, -0.2078, -0.0667,  ...,  0.5216,  0.4902,  0.4902]],\n",
       "\n",
       "         [[-0.6863, -0.7490, -0.7569,  ..., -0.7961, -0.8275, -0.9608],\n",
       "          [-0.6471, -0.7333, -0.6392,  ..., -0.7569, -0.8039, -0.9294],\n",
       "          [-0.7255, -0.7725, -0.4588,  ..., -0.4588, -0.7020, -0.7725],\n",
       "          ...,\n",
       "          [-0.1451,  0.1922,  0.4902,  ...,  0.4196,  0.5922,  0.5922],\n",
       "          [-0.5529, -0.2941,  0.0980,  ...,  0.1059,  0.1529,  0.2549],\n",
       "          [-0.6863, -0.6078, -0.4431,  ...,  0.2392,  0.1608,  0.1373]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2078,  0.2157,  0.2314,  ..., -0.0431, -0.0588, -0.0902],\n",
       "          [ 0.1922,  0.2000,  0.2078,  ..., -0.0039, -0.0275, -0.0510],\n",
       "          [ 0.2784,  0.2706,  0.2314,  ...,  0.0431,  0.0196,  0.0039],\n",
       "          ...,\n",
       "          [ 1.0000,  0.9843,  0.9765,  ...,  0.8824,  0.8667,  0.8588],\n",
       "          [ 1.0000,  0.9922,  0.9922,  ...,  0.9137,  0.9059,  0.8902],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9451,  0.9608,  0.9529]],\n",
       "\n",
       "         [[ 0.1843,  0.1922,  0.2078,  ..., -0.0980, -0.1137, -0.1373],\n",
       "          [ 0.1686,  0.1765,  0.1843,  ..., -0.0588, -0.0824, -0.0980],\n",
       "          [ 0.2549,  0.2392,  0.2078,  ..., -0.0118, -0.0353, -0.0431],\n",
       "          ...,\n",
       "          [ 1.0000,  0.9922,  0.9843,  ...,  0.8902,  0.8824,  0.8667],\n",
       "          [ 1.0000,  0.9922,  0.9922,  ...,  0.9216,  0.9137,  0.8980],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9529,  0.9686,  0.9608]],\n",
       "\n",
       "         [[ 0.0353,  0.0353,  0.0588,  ..., -0.2392, -0.2627, -0.2863],\n",
       "          [ 0.0196,  0.0275,  0.0353,  ..., -0.2000, -0.2235, -0.2471],\n",
       "          [ 0.1059,  0.0902,  0.0588,  ..., -0.1529, -0.1765, -0.1843],\n",
       "          ...,\n",
       "          [ 0.8980,  0.8745,  0.8745,  ...,  0.7490,  0.7412,  0.7255],\n",
       "          [ 0.9216,  0.9059,  0.9059,  ...,  0.7882,  0.7882,  0.7647],\n",
       "          [ 0.9529,  0.9294,  0.9373,  ...,  0.8431,  0.8588,  0.8353]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.8588, -0.8353, -0.8745,  ..., -0.6314, -0.7725, -0.7020],\n",
       "          [-0.8196, -0.8275, -0.8745,  ..., -0.6863, -0.7255, -0.5922],\n",
       "          [-0.8353, -0.8588, -0.8745,  ..., -0.7647, -0.6471, -0.4118],\n",
       "          ...,\n",
       "          [-0.3098, -0.3647, -0.3333,  ..., -0.1686, -0.1373, -0.1765],\n",
       "          [-0.2392, -0.2235, -0.1843,  ..., -0.2078, -0.2078, -0.1686],\n",
       "          [-0.1922, -0.1451, -0.0275,  ..., -0.0980, -0.1765, -0.2627]],\n",
       "\n",
       "         [[-0.8275, -0.8118, -0.8510,  ..., -0.5216, -0.7098, -0.6706],\n",
       "          [-0.7647, -0.7804, -0.8353,  ..., -0.6157, -0.7020, -0.5686],\n",
       "          [-0.7725, -0.8039, -0.8353,  ..., -0.7176, -0.6392, -0.4118],\n",
       "          ...,\n",
       "          [ 0.0118, -0.0118,  0.0039,  ...,  0.1451,  0.1765,  0.1216],\n",
       "          [ 0.0353,  0.0824,  0.1137,  ...,  0.1216,  0.1294,  0.1686],\n",
       "          [ 0.0431,  0.1216,  0.2314,  ...,  0.2000,  0.1294,  0.0353]],\n",
       "\n",
       "         [[-0.8824, -0.8588, -0.8902,  ..., -0.6706, -0.8275, -0.7882],\n",
       "          [-0.8431, -0.8353, -0.8745,  ..., -0.7725, -0.8431, -0.7098],\n",
       "          [-0.8353, -0.8510, -0.8824,  ..., -0.8588, -0.8039, -0.5765],\n",
       "          ...,\n",
       "          [-0.6863, -0.7412, -0.7333,  ..., -0.5373, -0.5059, -0.5765],\n",
       "          [-0.6000, -0.6000, -0.5686,  ..., -0.6000, -0.5765, -0.5608],\n",
       "          [-0.5451, -0.4902, -0.3569,  ..., -0.5216, -0.5686, -0.6471]]],\n",
       "\n",
       "\n",
       "        [[[-0.6314, -0.2863, -0.1843,  ..., -0.9765, -0.9843, -0.9843],\n",
       "          [-0.6627, -0.2078, -0.0902,  ..., -0.9922, -0.9608, -0.9765],\n",
       "          [-0.7725, -0.4510, -0.1294,  ..., -0.8196, -0.9843, -0.9686],\n",
       "          ...,\n",
       "          [-0.0275, -0.2784, -0.6784,  ...,  0.4275,  0.4039,  0.4275],\n",
       "          [-0.0196, -0.2471, -0.5686,  ...,  0.4118,  0.3961,  0.4196],\n",
       "          [-0.2078, -0.3255, -0.4745,  ...,  0.3647,  0.3961,  0.4118]],\n",
       "\n",
       "         [[-0.5686, -0.1059,  0.0745,  ..., -0.9922, -0.9922, -0.9922],\n",
       "          [-0.6863, -0.0118,  0.1608,  ..., -1.0000, -1.0000, -0.9922],\n",
       "          [-0.8353, -0.3333,  0.1529,  ..., -0.7412, -0.9922, -0.9922],\n",
       "          ...,\n",
       "          [ 0.0667, -0.2235, -0.6392,  ...,  0.3176,  0.3098,  0.3333],\n",
       "          [ 0.0431, -0.1843, -0.5216,  ...,  0.2941,  0.2941,  0.3255],\n",
       "          [-0.1137, -0.2314, -0.3725,  ...,  0.2627,  0.3255,  0.3255]],\n",
       "\n",
       "         [[-0.4588,  0.1843,  0.4275,  ..., -1.0000, -0.9843, -0.9608],\n",
       "          [-0.6392,  0.2784,  0.4980,  ..., -0.9765, -1.0000, -0.9922],\n",
       "          [-0.8196, -0.0902,  0.5216,  ..., -0.6000, -0.9529, -1.0000],\n",
       "          ...,\n",
       "          [ 0.2784, -0.0431, -0.5216,  ...,  0.3882,  0.3804,  0.3804],\n",
       "          [ 0.2941,  0.0196, -0.3255,  ...,  0.3569,  0.3647,  0.3725],\n",
       "          [ 0.1686,  0.0275, -0.1137,  ...,  0.3490,  0.3961,  0.3725]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6078,  0.3569,  0.3098,  ...,  0.3804,  0.3647,  0.3333],\n",
       "          [ 0.6314,  0.4196,  0.4431,  ...,  0.5216,  0.4902,  0.4667],\n",
       "          [ 0.6314,  0.4275,  0.4588,  ...,  0.5843,  0.5137,  0.4745],\n",
       "          ...,\n",
       "          [-0.3804, -0.8431, -0.8039,  ...,  0.2392,  0.1686,  0.1922],\n",
       "          [-0.4039, -0.7961, -0.7961,  ...,  0.1765,  0.1686,  0.2314],\n",
       "          [-0.4275, -0.7020, -0.7647,  ...,  0.1843,  0.1686,  0.2235]],\n",
       "\n",
       "         [[ 0.5765,  0.2941,  0.2235,  ...,  0.3020,  0.3176,  0.3098],\n",
       "          [ 0.5922,  0.3490,  0.3569,  ...,  0.4275,  0.4275,  0.4196],\n",
       "          [ 0.5922,  0.3647,  0.3725,  ...,  0.4667,  0.4275,  0.4118],\n",
       "          ...,\n",
       "          [ 0.3725, -0.0039, -0.1608,  ...,  0.2000,  0.1294,  0.1451],\n",
       "          [ 0.2863,  0.0118, -0.1216,  ...,  0.1529,  0.1451,  0.2078],\n",
       "          [ 0.1451, -0.0039, -0.1451,  ...,  0.1765,  0.1686,  0.2157]],\n",
       "\n",
       "         [[ 0.2392,  0.0353, -0.0275,  ...,  0.0588,  0.0667,  0.0510],\n",
       "          [ 0.2549,  0.0902,  0.1059,  ...,  0.1843,  0.1843,  0.1686],\n",
       "          [ 0.2549,  0.1059,  0.1216,  ...,  0.2314,  0.1922,  0.1686],\n",
       "          ...,\n",
       "          [-0.5373, -0.9294, -0.8980,  ..., -0.0902, -0.1608, -0.1373],\n",
       "          [-0.5922, -0.9059, -0.8824,  ..., -0.1373, -0.1451, -0.0824],\n",
       "          [-0.6549, -0.8510, -0.8745,  ..., -0.1216, -0.1294, -0.0824]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This creates an iterator from an iterable object.\n",
    "#Fetches the next element from the iterator.\n",
    "\n",
    "# 1st batch of 32 images\n",
    "print(next(iter(train_loader))[0].shape)\n",
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d232d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67fdda0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f08458",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b8d4322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe820a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f6b359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._MultiProcessingDataLoaderIter at 0x700ed5e94b50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get one batch\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1daa40a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44912caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdb637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4418d6",
   "metadata": {},
   "source": [
    "#### 2. Send to device\n",
    "PyTorch supports running tensors either on:\n",
    "\n",
    "CPU ‚Üí default if you're not using a GPU\n",
    "\n",
    "GPU (CUDA) ‚Üí much faster for deep learning\n",
    "\n",
    "#### images.to(device) will -\n",
    "‚û°Ô∏è \"Move this tensor (images) to the computing device (device) you specify ‚Äî either CPU or GPU.\"\n",
    "\n",
    "#### Hardware level\n",
    "\n",
    "üíª 1. Your CPU and GPU Have Separate Memory\n",
    "Component\tType of Memory\tExample Usage\n",
    "CPU\tRAM (main memory)\timages tensor when first loaded\n",
    "GPU\tVRAM (device memory)\timages.to(\"cuda\") sends it here\n",
    "\n",
    "| Component | Type of Memory       | Example Usage                     |\n",
    "| --------- | -------------------- | --------------------------------- |\n",
    "| CPU       | RAM (main memory)    | `images` tensor when first loaded |\n",
    "| GPU       | VRAM (device memory) | `images.to(\"cuda\")` sends it here |\n",
    "\n",
    "\n",
    "The CPU and GPU do not share memory. So if you want to use the GPU to compute something, you must copy the tensor from CPU RAM to GPU VRAM.\n",
    "\n",
    "‚öôÔ∏è 2. What Happens Internally?\n",
    "Here‚Äôs the process when you run tensor.to(\"cuda\"):\n",
    "\n",
    "PyTorch checks which device the tensor is currently on.\n",
    "\n",
    "Allocates memory in the GPU VRAM to hold the tensor.\n",
    "\n",
    "Copies data from system RAM to GPU memory using the CUDA driver.\n",
    "\n",
    "Returns a new tensor that lives on the GPU.\n",
    "\n",
    "This data transfer uses the PCIe (Peripheral Component Interconnect Express) bus ‚Äî this is the \"highway\" that connects your CPU and GPU physically.\n",
    "\n",
    "\n",
    "Slow: Moving data over PCIe is relatively slow. So if you do .to(\"cuda\") inside your training loop (on every batch), it will kill your performance.\n",
    "\n",
    "Plan ahead: Always move both your model and data to the GPU before training or inference begins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7223da7",
   "metadata": {},
   "source": [
    "#### `Run it before and after running .to(\"cuda\") ‚Äî you‚Äôll see the memory usage spike as your tensor lands in VRAM.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bee0b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e314cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images, labels = images.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a7d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae244a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.vit import VisionTransformer\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DataLoaderFactory\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8606872",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = VisionTransformer(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['train']['epochs']}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93081416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
