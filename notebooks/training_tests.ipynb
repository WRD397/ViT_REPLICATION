{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0ef90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "CURR_PATH = f'/home/wd/Documents/work_stuff/ViT_REPLICATION'\n",
    "sys.path.append(os.path.abspath(CURR_PATH))  # Adds root directory to sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11a34f",
   "metadata": {},
   "source": [
    "# Model Architecture and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f67d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# config numbers\n",
    "class ViTConfig:\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 in_channels=3,\n",
    "                 emb_size=64,\n",
    "                 depth=6,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.1):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_size = emb_size\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)        # (B, emb_size, H/patch, W/patch)\n",
    "        x = x.flatten(2)              # (B, emb_size, N)\n",
    "        x = x.transpose(1, 2)         # (B, N, emb_size)\n",
    "        return x                      # Shape: (B, N, E)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Vit Test Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        cfg = config[\"model\"]\n",
    "        \n",
    "        self.CHANNEL = cfg[\"in_channels\"]\n",
    "        self.PATCH = cfg[\"patch_size\"]\n",
    "        self.EMBEDDING = cfg[\"emb_size\"]\n",
    "        self.IMAGE = cfg[\"img_size\"]\n",
    "        self.NUM_HEADS = cfg[\"num_heads\"]\n",
    "        self.MLP_RATIO = cfg[\"mlp_ratio\"]\n",
    "        self.DROPOUT = cfg[\"dropout\"]\n",
    "        self.NUM_CLASS = cfg[\"num_classes\"]\n",
    "        self.DEPTH = cfg[\"depth\"]\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=self.CHANNEL,\n",
    "            patch_size=self.PATCH,\n",
    "            emb_size=self.EMBEDDING,\n",
    "            img_size=self.IMAGE\n",
    "        )\n",
    "\n",
    "        self.n_patches = (self.IMAGE// self.PATCH) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.EMBEDDING))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, self.EMBEDDING))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=self.EMBEDDING,\n",
    "                num_heads=self.NUM_HEADS,\n",
    "                mlp_ratio=self.MLP_RATIO,\n",
    "                dropout=self.DROPOUT\n",
    "            ) for _ in range(self.DEPTH)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.EMBEDDING)\n",
    "        self.head = nn.Linear(self.EMBEDDING, self.NUM_CLASS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        cls_out = self.norm(x[:, 0])\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8828f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model sanity\n",
      "torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 10])\n",
      "loading training testing data\n",
      "Train batches: 782, Test batches: 157\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([64, 3, 32, 32]), torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DatasetLoader\n",
    "\n",
    "# loading config file for model\n",
    "config = load_config(f\"{CURR_PATH}/config/vit_test_config.yaml\")\n",
    "model = VisionTransformer(config)\n",
    "\n",
    "print('testing model sanity')\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "print(dummy_input.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([1, 10])\n",
    "\n",
    "\n",
    "print('loading training testing data')\n",
    "# loading config file for CIFAR10\n",
    "data_cfg = config[\"data\"]\n",
    "DATASET = data_cfg[\"dataset\"]\n",
    "DATA_DIR = data_cfg[\"data_path\"]\n",
    "BATCH = data_cfg[\"batch_size\"]\n",
    "NUM_WORKERS = data_cfg[\"num_workers\"]\n",
    "IMAGE = data_cfg[\"img_size\"]\n",
    "\n",
    "# loading data\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, test_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb150eed",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae244a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.vit import VisionTransformer\n",
    "from utils.config_loader import load_config\n",
    "from utils.data_loader import DataLoaderFactory\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8606872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = VisionTransformer(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"train\"][\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config[\"train\"][\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['train']['epochs']}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93081416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
