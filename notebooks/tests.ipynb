{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "643b97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8f6ac",
   "metadata": {},
   "source": [
    "# Data - CIFAR10\n",
    "have 10 classes - labelled 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db5622d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data which is already there in torchvision repo\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataPath = f'/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10'\n",
    "dataset = torchvision.datasets.CIFAR10(root=dataPath, train=False, download=True,  \n",
    "                                        transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30b1d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89218a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6196, 0.6235, 0.6471,  ..., 0.5373, 0.4941, 0.4549],\n",
       "         [0.5961, 0.5922, 0.6235,  ..., 0.5333, 0.4902, 0.4667],\n",
       "         [0.5922, 0.5922, 0.6196,  ..., 0.5451, 0.5098, 0.4706],\n",
       "         ...,\n",
       "         [0.2667, 0.1647, 0.1216,  ..., 0.1490, 0.0510, 0.1569],\n",
       "         [0.2392, 0.1922, 0.1373,  ..., 0.1020, 0.1137, 0.0784],\n",
       "         [0.2118, 0.2196, 0.1765,  ..., 0.0941, 0.1333, 0.0824]],\n",
       "\n",
       "        [[0.4392, 0.4353, 0.4549,  ..., 0.3725, 0.3569, 0.3333],\n",
       "         [0.4392, 0.4314, 0.4471,  ..., 0.3725, 0.3569, 0.3451],\n",
       "         [0.4314, 0.4275, 0.4353,  ..., 0.3843, 0.3725, 0.3490],\n",
       "         ...,\n",
       "         [0.4863, 0.3922, 0.3451,  ..., 0.3804, 0.2510, 0.3333],\n",
       "         [0.4549, 0.4000, 0.3333,  ..., 0.3216, 0.3216, 0.2510],\n",
       "         [0.4196, 0.4118, 0.3490,  ..., 0.3020, 0.3294, 0.2627]],\n",
       "\n",
       "        [[0.1922, 0.1843, 0.2000,  ..., 0.1412, 0.1412, 0.1294],\n",
       "         [0.2000, 0.1569, 0.1765,  ..., 0.1216, 0.1255, 0.1333],\n",
       "         [0.1843, 0.1294, 0.1412,  ..., 0.1333, 0.1333, 0.1294],\n",
       "         ...,\n",
       "         [0.6941, 0.5804, 0.5373,  ..., 0.5725, 0.4235, 0.4980],\n",
       "         [0.6588, 0.5804, 0.5176,  ..., 0.5098, 0.4941, 0.4196],\n",
       "         [0.6275, 0.5843, 0.5176,  ..., 0.4863, 0.5059, 0.4314]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "856fd4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8752e9",
   "metadata": {},
   "source": [
    "\n",
    "### img.numpy()  vs  np.array(img)\n",
    "For example, the tensor is -> img.shape = torch.Size([3, 32, 32])\n",
    "\n",
    "#### img.numpy() :::: What it does:\n",
    "\n",
    "Converts the PyTorch Tensor to a NumPy array.\n",
    "\n",
    "Shares the same memory (zero-copy).\n",
    "\n",
    "Only works if the tensor is on CPU.\n",
    "\n",
    "‚úÖ Fast and efficient.\n",
    "‚ö†Ô∏è Will raise an error if the tensor is on GPU.\n",
    "\n",
    "img = img.cpu()  # Must be on CPU\n",
    "img_np = img.numpy()\n",
    "\n",
    "#### np.array(img) :::: What it does:\n",
    "What it does:\n",
    "\n",
    "Uses NumPy to create an array from the tensor.\n",
    "\n",
    "Copies the data ‚Äî does not share memory.\n",
    "\n",
    "Slower and not recommended for tensors.\n",
    "\n",
    "Works even if the tensor is on GPU (but will be wrong!).\n",
    "\n",
    "‚ö†Ô∏è Inefficient.\n",
    "‚ö†Ô∏è May silently produce unexpected results or copy issues.\n",
    "\n",
    "\n",
    "| Method          | Converts To NumPy | Shares Memory? | Fast? | GPU Compatible?      | Recommended? |\n",
    "| --------------- | ----------------- | -------------- | ----- | -------------------- | ------------ |\n",
    "| `img.numpy()`   | ‚úÖ                 | ‚úÖ Yes          | ‚úÖ Yes | ‚ùå No (CPU only)      | ‚úÖ Yes        |\n",
    "| `np.array(img)` | ‚úÖ                 | ‚ùå No (copy)    | ‚ùå No  | ‚ö†Ô∏è Can silently fail | ‚ùå No         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0550b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.61960787, 0.62352943, 0.64705884, ..., 0.5372549 ,\n",
       "         0.49411765, 0.45490196],\n",
       "        [0.59607846, 0.5921569 , 0.62352943, ..., 0.53333336,\n",
       "         0.49019608, 0.46666667],\n",
       "        [0.5921569 , 0.5921569 , 0.61960787, ..., 0.54509807,\n",
       "         0.50980395, 0.47058824],\n",
       "        ...,\n",
       "        [0.26666668, 0.16470589, 0.12156863, ..., 0.14901961,\n",
       "         0.05098039, 0.15686275],\n",
       "        [0.23921569, 0.19215687, 0.13725491, ..., 0.10196079,\n",
       "         0.11372549, 0.07843138],\n",
       "        [0.21176471, 0.21960784, 0.1764706 , ..., 0.09411765,\n",
       "         0.13333334, 0.08235294]],\n",
       "\n",
       "       [[0.4392157 , 0.43529412, 0.45490196, ..., 0.37254903,\n",
       "         0.35686275, 0.33333334],\n",
       "        [0.4392157 , 0.43137255, 0.44705883, ..., 0.37254903,\n",
       "         0.35686275, 0.34509805],\n",
       "        [0.43137255, 0.42745098, 0.43529412, ..., 0.38431373,\n",
       "         0.37254903, 0.34901962],\n",
       "        ...,\n",
       "        [0.4862745 , 0.39215687, 0.34509805, ..., 0.38039216,\n",
       "         0.2509804 , 0.33333334],\n",
       "        [0.45490196, 0.4       , 0.33333334, ..., 0.32156864,\n",
       "         0.32156864, 0.2509804 ],\n",
       "        [0.41960785, 0.4117647 , 0.34901962, ..., 0.3019608 ,\n",
       "         0.32941177, 0.2627451 ]],\n",
       "\n",
       "       [[0.19215687, 0.18431373, 0.2       , ..., 0.14117648,\n",
       "         0.14117648, 0.12941177],\n",
       "        [0.2       , 0.15686275, 0.1764706 , ..., 0.12156863,\n",
       "         0.1254902 , 0.13333334],\n",
       "        [0.18431373, 0.12941177, 0.14117648, ..., 0.13333334,\n",
       "         0.13333334, 0.12941177],\n",
       "        ...,\n",
       "        [0.69411767, 0.5803922 , 0.5372549 , ..., 0.57254905,\n",
       "         0.42352942, 0.49803922],\n",
       "        [0.65882355, 0.5803922 , 0.5176471 , ..., 0.50980395,\n",
       "         0.49411765, 0.41960785],\n",
       "        [0.627451  , 0.58431375, 0.5176471 , ..., 0.4862745 ,\n",
       "         0.5058824 , 0.43137255]]], shape=(3, 32, 32), dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87bf7937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(img.numpy(), (1, 2, 0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb1c1d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0))) # you need to pass the index : channel, height, width -> height, width, channel\n",
    "plt.title(f\"Class: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb9ecff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = dataset[9999]\n",
    "plt.imshow(np.transpose(img.numpy(), (1, 2, 0))) # you need to pass the index : channel, height, width -> height, width, channel\n",
    "plt.title(f\"Class: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb525537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "920874f1",
   "metadata": {},
   "source": [
    "# Patch Embedding\n",
    "üì¶ What is the Goal?\n",
    "We want to:\n",
    "\n",
    "Break an image into non-overlapping patches (e.g., 4√ó4).\n",
    "\n",
    "Flatten each patch.\n",
    "\n",
    "Project each patch into a vector space (i.e., embedding).\n",
    "\n",
    "Output shape should be (Batch, Num_Patches, Embedding_Dim) ‚Äî just like how Transformers expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148eeb7",
   "metadata": {},
   "source": [
    "\n",
    "### whats the significance of conv2d here ?\n",
    "\n",
    "üéØ What are we trying to do?\n",
    "We want to convert an image (e.g., 32√ó32) into a sequence of patch vectors, where each patch is represented by a fixed-size vector (like 64-dim, 128-dim etc). This is necessary because:\n",
    "\n",
    "Transformers expect input as a sequence of vectors: (batch, num_patches, embedding_dim)\n",
    "\n",
    "So we need some way to go from image pixels ‚Üí patch vectors.\n",
    "\n",
    "üß† This single Conv2d call:\n",
    "\n",
    "Extracts non-overlapping patches\n",
    "\n",
    "Flattens each patch internally\n",
    "\n",
    "Applies a learnable linear transformation to each patch\n",
    "\n",
    "So it acts like a smart patch extractor + embedder in one step.\n",
    "\n",
    "üîç Why Conv2D works perfectly here\n",
    "kernel_size = patch_size: it looks at one patch at a time.\n",
    "\n",
    "stride = patch_size: it moves in steps of patch size (no overlap).\n",
    "\n",
    "out_channels = emb_size: it maps each patch to an emb_size-d vector.\n",
    "\n",
    "This is exactly what we want:\n",
    "\n",
    "Divide the image into fixed-size patches, and turn each patch into a vector.\n",
    "\n",
    "### üß™ Without Conv2D ‚Äî what would you do?\n",
    "\n",
    "If you didn‚Äôt use Conv2d, you‚Äôd need to:\n",
    "\n",
    "Slice the image manually into 4√ó4 patches (nested loops).\n",
    "\n",
    "Flatten each patch into a vector.\n",
    "\n",
    "Apply a learnable linear projection (nn.Linear) to each one.\n",
    "\n",
    "That‚Äôs a LOT of code and slow on GPU.\n",
    "\n",
    "‚úÖ Conv2d gives you all that in one fast, GPU-optimized layer.\n",
    "\n",
    "üîÅ Think of Conv2D here not as a CNN, but as a fast way to do:\n",
    "Patch ‚Üí Vector projection for all patches at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d55d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)       # (B, emb_size, H/patch, W/patch)\n",
    "        print(f'after applying conv2D, {x.shape}')\n",
    "        x = x.flatten(2)             # (B, emb_size, N)\n",
    "        print(f'after flattening, {x.shape}')\n",
    "        x = x.transpose(1, 2)        # (B, N, emb_size)\n",
    "        print(f'after transpose {x.shape}')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "226fe69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2863, 0.3843, 0.3882,  ..., 0.5294, 0.5294, 0.7961],\n",
       "          [0.2706, 0.3294, 0.2667,  ..., 0.3333, 0.2784, 0.4706],\n",
       "          [0.2706, 0.3529, 0.2431,  ..., 0.2902, 0.2078, 0.2431],\n",
       "          ...,\n",
       "          [0.4824, 0.5176, 0.5059,  ..., 0.4235, 0.2431, 0.1059],\n",
       "          [0.4510, 0.4824, 0.5059,  ..., 0.4510, 0.2588, 0.1059],\n",
       "          [0.4549, 0.4745, 0.5059,  ..., 0.4549, 0.2667, 0.1059]],\n",
       "\n",
       "         [[0.3059, 0.4039, 0.4157,  ..., 0.5882, 0.5843, 0.8431],\n",
       "          [0.2863, 0.3490, 0.2941,  ..., 0.3725, 0.3216, 0.5216],\n",
       "          [0.2863, 0.3725, 0.2784,  ..., 0.3176, 0.2431, 0.2902],\n",
       "          ...,\n",
       "          [0.5020, 0.5176, 0.5020,  ..., 0.4196, 0.2353, 0.1059],\n",
       "          [0.4745, 0.4863, 0.4941,  ..., 0.4549, 0.2549, 0.1059],\n",
       "          [0.4706, 0.4784, 0.5020,  ..., 0.4510, 0.2549, 0.1020]],\n",
       "\n",
       "         [[0.2941, 0.4431, 0.4471,  ..., 0.5961, 0.6039, 0.8745],\n",
       "          [0.2745, 0.3804, 0.3176,  ..., 0.3490, 0.3137, 0.5294],\n",
       "          [0.2745, 0.3922, 0.2902,  ..., 0.2745, 0.2118, 0.2706],\n",
       "          ...,\n",
       "          [0.3765, 0.4000, 0.3922,  ..., 0.3451, 0.2157, 0.1098],\n",
       "          [0.3569, 0.3725, 0.3882,  ..., 0.3686, 0.2314, 0.1059],\n",
       "          [0.3529, 0.3686, 0.3961,  ..., 0.3686, 0.2275, 0.1020]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## UNSQUEEZE : Adds a dimension of size 1 at the specified dim (axis).\n",
    "img_tensor = img.unsqueeze(0)\n",
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "154349fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bb6adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 4\n",
    "n_patches = (32 // patch_size) ** 2\n",
    "projection = nn.Conv2d(3, 128, kernel_size=patch_size, stride=patch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d22fd5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2863, 0.3843, 0.3882,  ..., 0.5294, 0.5294, 0.7961],\n",
       "          [0.2706, 0.3294, 0.2667,  ..., 0.3333, 0.2784, 0.4706],\n",
       "          [0.2706, 0.3529, 0.2431,  ..., 0.2902, 0.2078, 0.2431],\n",
       "          ...,\n",
       "          [0.4824, 0.5176, 0.5059,  ..., 0.4235, 0.2431, 0.1059],\n",
       "          [0.4510, 0.4824, 0.5059,  ..., 0.4510, 0.2588, 0.1059],\n",
       "          [0.4549, 0.4745, 0.5059,  ..., 0.4549, 0.2667, 0.1059]],\n",
       "\n",
       "         [[0.3059, 0.4039, 0.4157,  ..., 0.5882, 0.5843, 0.8431],\n",
       "          [0.2863, 0.3490, 0.2941,  ..., 0.3725, 0.3216, 0.5216],\n",
       "          [0.2863, 0.3725, 0.2784,  ..., 0.3176, 0.2431, 0.2902],\n",
       "          ...,\n",
       "          [0.5020, 0.5176, 0.5020,  ..., 0.4196, 0.2353, 0.1059],\n",
       "          [0.4745, 0.4863, 0.4941,  ..., 0.4549, 0.2549, 0.1059],\n",
       "          [0.4706, 0.4784, 0.5020,  ..., 0.4510, 0.2549, 0.1020]],\n",
       "\n",
       "         [[0.2941, 0.4431, 0.4471,  ..., 0.5961, 0.6039, 0.8745],\n",
       "          [0.2745, 0.3804, 0.3176,  ..., 0.3490, 0.3137, 0.5294],\n",
       "          [0.2745, 0.3922, 0.2902,  ..., 0.2745, 0.2118, 0.2706],\n",
       "          ...,\n",
       "          [0.3765, 0.4000, 0.3922,  ..., 0.3451, 0.2157, 0.1098],\n",
       "          [0.3569, 0.3725, 0.3882,  ..., 0.3686, 0.2314, 0.1059],\n",
       "          [0.3529, 0.3686, 0.3961,  ..., 0.3686, 0.2275, 0.1020]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2221a82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0202, -0.1045, -0.0300,  ..., -0.0988, -0.3745, -0.1823],\n",
       "          [-0.0394,  0.0039, -0.0760,  ..., -0.1092, -0.0894, -0.0464],\n",
       "          [ 0.0695,  0.0468, -0.0259,  ..., -0.0219, -0.0361, -0.1044],\n",
       "          ...,\n",
       "          [-0.0519, -0.0913,  0.0129,  ..., -0.1006, -0.1024, -0.1740],\n",
       "          [-0.0485, -0.1100,  0.0042,  ..., -0.0741, -0.0532, -0.1300],\n",
       "          [-0.0783, -0.1298, -0.0687,  ..., -0.0541, -0.0476, -0.1475]],\n",
       "\n",
       "         [[ 0.2131,  0.2673,  0.3952,  ...,  0.3206,  0.2571,  0.3463],\n",
       "          [ 0.1800,  0.1600,  0.1847,  ...,  0.2684,  0.1805,  0.1569],\n",
       "          [ 0.1249,  0.1527,  0.1621,  ...,  0.2292,  0.2173,  0.2119],\n",
       "          ...,\n",
       "          [ 0.2243,  0.2630,  0.1739,  ...,  0.2378,  0.2373,  0.1576],\n",
       "          [ 0.2171,  0.1917,  0.1826,  ...,  0.2284,  0.2200,  0.1457],\n",
       "          [ 0.2292,  0.2096,  0.2355,  ...,  0.2133,  0.2065,  0.1290]],\n",
       "\n",
       "         [[-0.0165,  0.0415,  0.0923,  ..., -0.0759, -0.1321,  0.1253],\n",
       "          [ 0.0247, -0.0124, -0.0133,  ...,  0.0579, -0.1140,  0.0196],\n",
       "          [ 0.0422,  0.0170,  0.0253,  ...,  0.0097, -0.0597, -0.0055],\n",
       "          ...,\n",
       "          [-0.0485,  0.0862,  0.0254,  ..., -0.0300, -0.0342, -0.1687],\n",
       "          [-0.0048, -0.0372,  0.0147,  ..., -0.0303, -0.0597, -0.1229],\n",
       "          [-0.0374, -0.0234,  0.0162,  ..., -0.0164, -0.0274, -0.1390]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1694, -0.1645, -0.2404,  ..., -0.4103, -0.4106, -0.2891],\n",
       "          [-0.1764, -0.1679, -0.1752,  ..., -0.2501, -0.2661, -0.1908],\n",
       "          [-0.2979, -0.2614, -0.2837,  ..., -0.3312, -0.3008, -0.3260],\n",
       "          ...,\n",
       "          [-0.2652, -0.2947, -0.1519,  ..., -0.2803, -0.2798, -0.2771],\n",
       "          [-0.2476, -0.2915, -0.2010,  ..., -0.2760, -0.2674, -0.2330],\n",
       "          [-0.2768, -0.2761, -0.2745,  ..., -0.2585, -0.2608, -0.2329]],\n",
       "\n",
       "         [[ 0.0891,  0.1886,  0.1712,  ...,  0.1097,  0.2200,  0.1955],\n",
       "          [ 0.1231,  0.1019,  0.1177,  ...,  0.1113,  0.0155,  0.1025],\n",
       "          [-0.0439, -0.0112,  0.0390,  ...,  0.1490,  0.1900,  0.1637],\n",
       "          ...,\n",
       "          [ 0.1537,  0.1861,  0.1930,  ...,  0.1562,  0.1663,  0.1099],\n",
       "          [ 0.1580,  0.1056,  0.1273,  ...,  0.1423,  0.1266,  0.0889],\n",
       "          [ 0.1521,  0.1758,  0.1689,  ...,  0.0984,  0.1126,  0.0720]],\n",
       "\n",
       "         [[-0.0701, -0.0985, -0.1588,  ...,  0.0180, -0.0139, -0.2165],\n",
       "          [-0.0885, -0.0641, -0.0702,  ..., -0.1353, -0.2739, -0.1109],\n",
       "          [-0.0649, -0.0187,  0.0019,  ..., -0.1564, -0.1462, -0.1163],\n",
       "          ...,\n",
       "          [-0.0840, -0.0980, -0.0651,  ..., -0.0873, -0.0844, -0.0435],\n",
       "          [-0.0381, -0.0930, -0.0884,  ..., -0.0848, -0.0781, -0.0728],\n",
       "          [-0.0881, -0.0717, -0.1173,  ..., -0.0878, -0.0726, -0.0592]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### applying convolution\n",
    "x = projection(img_tensor)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cb60eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 8, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15a2520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88e1d60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten(2).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a665ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after applying conv2D, torch.Size([1, 128, 8, 8])\n",
      "after flattening, torch.Size([1, 128, 64])\n",
      "after transpose torch.Size([1, 64, 128])\n",
      "Output shape: torch.Size([1, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = img.unsqueeze(0)  # Add batch dimension: for one image, the shape will be (1, 3, 32, 32), we are faking the batch size just because the model demands a 4D tensor as the input\n",
    "\n",
    "patch_embed = PatchEmbedding(patch_size=4, emb_size=128)\n",
    "with torch.no_grad():\n",
    "    patch_vectors = patch_embed(img_tensor)\n",
    "\n",
    "print(\"Output shape:\", patch_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bef83d",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "### ‚ùì Why is this needed?\n",
    "Transformers don‚Äôt ‚Äúsee‚Äù structure in sequences ‚Äî they treat them as a bag of vectors. So if we feed in 64 patches, it doesn't know:\n",
    "\n",
    "Which patch is top-left?\n",
    "\n",
    "Which one is center?\n",
    "\n",
    "Which one is bottom-right?\n",
    "\n",
    "‚û°Ô∏è We solve this by adding positional information to each patch vector.\n",
    "\n",
    "Transformers don‚Äôt know anything about order or position in sequences unless we tell them. Unlike CNNs, which use kernels that scan spatially across pixels, Transformers treat inputs as just a set of tokens.\n",
    "\n",
    "In ViT:\n",
    "We give the Transformer a list of 64 patch vectors.\n",
    "But... it has no clue which patch came from where on the image. üòµ\n",
    "\n",
    "So we need to inject position awareness using positional embeddings.\n",
    "\n",
    "### ‚úÖ It‚Äôs just a tensor added to each patch vector:\n",
    "patch_embedding:    (1, 64, 128) ‚Üí 64 patches, each 128-dim\n",
    "positional_encoding:(1, 64, 128) ‚Üí 64 positional vectors\n",
    "positioned = patch_embedding + positional_encoding\n",
    "\n",
    "So the final input to the Transformer is:\n",
    "(1, 64, 128) ‚Äî where each patch vector now includes position info\n",
    "\n",
    "### üß† Two Ways to Do This\n",
    "Method\tDescription\tCommon in\n",
    "- Fixed Sinusoidal\tUses sine/cosine functions\tNLP Transformers (e.g., BERT)\n",
    "- ‚úÖ Learnable\tPositional embeddings are learned during training\tViT, modern vision models\n",
    "\n",
    "We‚Äôll focus on the learnable version, as used in Vision Transformers.\n",
    "self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_size)) , num_patches + 1 is to include 'cls_token'.\n",
    "\n",
    "| Patch Index | Patch Embedding (128D)   | Positional Embedding (128D) | Final Input to Transformer (128D) |\n",
    "| ----------- | ----------------------- | -------------------------- | -------------------------------- |\n",
    "| Patch 0     | `[e‚ÇÄ‚ÇÄ, e‚ÇÄ‚ÇÅ, ..., e‚ÇÄ127]` | `[p‚ÇÄ‚ÇÄ, p‚ÇÄ‚ÇÅ, ..., p‚ÇÄ127]`    | `e‚ÇÄ + p‚ÇÄ`                        |\n",
    "| Patch 1     | `[e‚ÇÅ‚ÇÄ, e‚ÇÅ‚ÇÅ, ..., e‚ÇÅ127]` | `[p‚ÇÅ‚ÇÄ, p‚ÇÅ‚ÇÅ, ..., p‚ÇÅ127]`    | `e‚ÇÅ + p‚ÇÅ`                        |\n",
    "| ...         | ...                     | ...                        | ...                              |\n",
    "| Patch 63    | ...                     | ...                        | `e‚ÇÜ‚ÇÉ + p‚ÇÜ‚ÇÉ`                      |\n",
    "\n",
    "\n",
    "This way:\n",
    "\n",
    "Even if two patches have identical pixel values (same e·µ¢)\n",
    "\n",
    "Their final representation will be different because of p·µ¢\n",
    "\n",
    "‚úÖ That‚Äôs how we encode spatial awareness into a Transformer.\n",
    "\n",
    "\n",
    "### CLS TOKEN\n",
    "It‚Äôs a special trainable vector that is prepended to the sequence of patch embeddings before feeding into the Transformer.\n",
    "\n",
    "Originally introduced in BERT (NLP model), it‚Äôs reused in ViT.\n",
    "\n",
    "Transformers process entire sequences, and you typically get one output per token.\n",
    "But in classification, you want one output for the entire image.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "Averaging the patch outputs (like in CNN global pooling),\n",
    "\n",
    "Or building a separate head,\n",
    "\n",
    "‚úÖ You just add a [CLS] token, and let the model learn to treat it as a summary collector.\n",
    "\n",
    "‚úÖ Summary Role of [CLS] Token\n",
    "Input: Added to start of patch sequence\n",
    "\n",
    "Transformer: Attends to all patches in every layer\n",
    "\n",
    "Output: Final hidden state of [CLS] is used for classification\n",
    "\n",
    "Think of [CLS] as a learnable sponge that absorbs global image information through attention.\n",
    "\n",
    "üî¨ How It Works in Practice\n",
    "\n",
    "üî¢ Suppose:\n",
    "Patch embedding produces 64 tokens of 128 dims each ‚Üí shape: (B, 64, 128)\n",
    "\n",
    "You prepend 1 [CLS] token ‚Üí now shape: (B, 65, 128)\n",
    "\n",
    "So:\n",
    "\n",
    "x.shape after patch embedding = (B, 64, 128)\n",
    "\n",
    "Add [CLS]: becomes (B, 65, 128)\n",
    "\n",
    "pos_embedding.shape: (1, 65, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47b500a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Output from the previous step - Patch Embedding \n",
    "patch_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8e54b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0281,  0.0488, -0.1671,  ...,  0.1449,  0.1038, -0.2975],\n",
       "         [ 0.0119,  0.0947,  0.0313,  ...,  0.1755, -0.0231, -0.4171],\n",
       "         [-0.0735,  0.2013, -0.1353,  ...,  0.2423, -0.0250, -0.5145],\n",
       "         ...,\n",
       "         [ 0.0921,  0.0644, -0.3029,  ...,  0.1410,  0.0883, -0.3464],\n",
       "         [ 0.0856,  0.0758, -0.3246,  ...,  0.1737,  0.0869, -0.3674],\n",
       "         [ 0.0298, -0.0794, -0.3328,  ..., -0.0320,  0.2554, -0.1896]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38558eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTInput(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_patches=64, emb_size=64, include_cls=True):    \n",
    "        super().__init__()\n",
    "        self.include_cls = include_cls\n",
    "        if include_cls:\n",
    "            # Create a trainable CLS token\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))  # (1, 1, 64)\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "        \n",
    "        # Positional embedding for all tokens (patches + CLS if present)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + int(include_cls), emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x shape: (B, 64, 64)        \n",
    "        B, N, E = x.shape\n",
    "        \n",
    "        if self.include_cls:\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, E)\n",
    "            x = torch.cat([cls_tokens, x], dim=1)          # (B, 65, E)\n",
    "\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]       # Add positional encoding\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.5443, -1.2087, -0.8276,  0.5431,  0.6837,  0.6449, -0.3903,\n",
       "           1.6828,  1.4183,  1.3319,  2.6950, -0.9619,  1.3587, -0.2581,\n",
       "          -0.2381, -0.1279,  0.8174, -0.0683, -0.3495, -0.3837, -1.0224,\n",
       "           1.7470, -1.3680,  1.4968,  1.0419,  1.8361,  1.3865,  0.3257,\n",
       "          -0.3368,  1.5277, -1.4472,  0.6737,  1.1420, -0.2218, -1.5787,\n",
       "           0.3148, -0.5148, -0.8375, -0.6938,  0.5123, -0.1617, -0.6039,\n",
       "          -0.1356, -0.5148, -0.1932, -1.5606,  0.8449, -2.3375, -1.3724,\n",
       "           0.2222,  0.7883,  1.3188, -0.3839, -1.1409,  0.6007,  1.1602,\n",
       "          -0.9717, -0.2907,  0.0898,  0.1581,  0.4025,  0.4153, -0.5192,\n",
       "           0.6851,  2.1454, -0.1097, -1.2872,  1.1641, -0.7736,  1.2010,\n",
       "           0.4821, -2.3750,  0.5498, -1.0623,  0.7939, -0.9616,  0.7300,\n",
       "           0.0193,  0.1770,  0.6844, -1.2934, -0.5852, -0.7679,  0.4909,\n",
       "          -0.2089,  0.0039,  0.2492,  0.4179, -0.8954, -1.0040,  1.3773,\n",
       "           0.4326,  0.3358,  0.4265, -0.7622, -1.5604, -0.0716,  0.5139,\n",
       "          -0.1304,  0.6168, -0.0513, -0.3771, -2.1280,  1.9902, -0.1103,\n",
       "           0.6034, -1.5095, -2.0950,  0.2384,  0.8864,  0.0380, -0.5656,\n",
       "           1.0347,  0.5639, -2.0511,  0.0844,  0.3589, -0.0933, -1.2440,\n",
       "          -0.1617,  2.0943,  1.4670, -0.2284, -0.5596,  1.5421,  1.3926,\n",
       "           0.3727, -0.2670]]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token = nn.Parameter(torch.randn(1, 1, 128))\n",
    "cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "141c0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f75e8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c073e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 128])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = 64\n",
    "include_cls = True\n",
    "emb_size=128\n",
    "\n",
    "B, N, E = patch_vectors.shape\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, 128))\n",
    "cls_tokens = cls_token.expand(B, -1, -1)\n",
    "patch_vectors_cls_added = torch.cat([cls_tokens, patch_vectors], dim=1)\n",
    "patch_vectors_cls_added.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b03fcd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6458,  1.0369, -1.0193,  ...,  1.7842, -0.1007,  0.2384],\n",
       "         [ 0.0281,  0.0488, -0.1671,  ...,  0.1449,  0.1038, -0.2975],\n",
       "         [ 0.0119,  0.0947,  0.0313,  ...,  0.1755, -0.0231, -0.4171],\n",
       "         ...,\n",
       "         [ 0.0921,  0.0644, -0.3029,  ...,  0.1410,  0.0883, -0.3464],\n",
       "         [ 0.0856,  0.0758, -0.3246,  ...,  0.1737,  0.0869, -0.3674],\n",
       "         [ 0.0298, -0.0794, -0.3328,  ..., -0.0320,  0.2554, -0.1896]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_vectors_cls_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da546c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.3151, -1.0432, -0.0393,  ..., -0.0140, -1.2721, -1.7620],\n",
       "         [ 1.3543, -0.7176, -1.4389,  ...,  0.2655, -1.0941,  1.2384],\n",
       "         [ 2.3830, -1.8270,  0.2797,  ..., -0.5667,  0.1631, -0.5172],\n",
       "         ...,\n",
       "         [-0.0358,  1.3445, -0.1702,  ..., -0.8810,  0.1896, -1.4092],\n",
       "         [ 0.8369, -0.0627, -0.4133,  ..., -1.0886,  1.2613, -0.3744],\n",
       "         [-0.1815, -0.4302,  2.4264,  ...,  1.7539,  1.8274,  1.6226]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding = nn.Parameter(\n",
    "        torch.randn(1, num_patches + int(include_cls), emb_size)\n",
    "    )\n",
    "pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fec7285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 128])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee8af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.3151, -1.0432, -0.0393,  ..., -0.0140, -1.2721, -1.7620],\n",
       "         [ 1.3543, -0.7176, -1.4389,  ...,  0.2655, -1.0941,  1.2384],\n",
       "         [ 2.3830, -1.8270,  0.2797,  ..., -0.5667,  0.1631, -0.5172],\n",
       "         ...,\n",
       "         [-0.0358,  1.3445, -0.1702,  ..., -0.8810,  0.1896, -1.4092],\n",
       "         [ 0.8369, -0.0627, -0.4133,  ..., -1.0886,  1.2613, -0.3744],\n",
       "         [-0.1815, -0.4302,  2.4264,  ...,  1.7539,  1.8274,  1.6226]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding[:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f238dde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_vectors_cls_added.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93fbd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = nn.Parameter(\n",
    "    torch.randn(1, num_patches + int(include_cls), emb_size)\n",
    "    )\n",
    "tokens_with_cls = patch_vectors_cls_added + pos_embedding[:, :patch_vectors_cls_added.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be3b441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6886,  0.0145, -1.1406,  ...,  0.9967, -0.1300,  0.2415],\n",
       "         [ 3.2012,  0.8436, -0.5777,  ..., -0.4499,  1.4338,  1.1878],\n",
       "         [ 0.7825, -0.2857,  0.4328,  ...,  0.9572,  1.8117, -1.2131],\n",
       "         ...,\n",
       "         [ 0.4234, -1.3387,  1.3495,  ...,  1.0028,  1.1487, -2.0295],\n",
       "         [-0.2278, -0.4514,  0.0765,  ...,  1.4625,  1.1042, -1.1956],\n",
       "         [ 0.9122,  0.0558, -1.8295,  ...,  1.7379, -0.2798,  0.5286]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_with_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a37771c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape with CLS: torch.Size([1, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "vit_input = ViTInput(num_patches=64, emb_size=128, include_cls=True)\n",
    "tokens_with_cls = vit_input(patch_vectors)\n",
    "print(\"Final shape with CLS:\", tokens_with_cls.shape)  # (1, 65, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc9a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4908b745",
   "metadata": {},
   "source": [
    "# Transformer Encoder Block\n",
    "üí° Each Transformer block contains:\n",
    "Multi-Head Self-Attention (MHSA)\n",
    "\n",
    "Add & LayerNorm\n",
    "\n",
    "Feedforward MLP\n",
    "\n",
    "Add & LayerNorm\n",
    "\n",
    "There are multiple such blocks like 6 or 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b3849ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_with_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size=64, num_heads=4, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Self-Attention with Residual\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        # Feedforward MLP with Residual\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b56d1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape from Transformer Encoder Block: torch.Size([1, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder_block = TransformerEncoderBlock(emb_size=128, num_heads=4)\n",
    "# Pass input tokens through Transformer Encoder Block\n",
    "with torch.no_grad():  # Disable gradients for now (inference/debug)\n",
    "    output_tokens = encoder_block(tokens_with_cls)\n",
    "\n",
    "print(\"Output shape from Transformer Encoder Block:\", output_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fcf4893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 128])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c190822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3190, -3.1615,  0.4054,  ...,  1.9170, -2.6788, -0.7548],\n",
       "         [-0.8424,  0.2914, -1.7966,  ...,  1.3436, -0.3509,  0.9910],\n",
       "         [ 0.5700, -0.3865, -0.0163,  ...,  0.1557, -2.4257, -0.8484],\n",
       "         ...,\n",
       "         [ 2.4268, -0.2838,  0.4983,  ...,  1.5367, -0.2998,  1.0226],\n",
       "         [-1.2276,  2.2232,  0.0259,  ..., -0.9833, -0.1569, -0.0787],\n",
       "         [-0.4944, -1.6785,  0.2877,  ...,  0.5237, -1.2672,  0.2341]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de0e8a",
   "metadata": {},
   "source": [
    "# Extracting CLS Token for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b573b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.fc = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = x[:, 0]       # Extract only the [CLS] token: shape (B, E)\n",
    "        cls_token = self.norm(cls_token)\n",
    "        return self.fc(cls_token)  # Final shape: (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f7e3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "head = ClassificationHead(emb_size=128, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    logits = head(output_tokens)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fee240e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5009, -0.0218,  0.1075,  0.9305, -0.6950, -0.1372,  0.2581, -0.5056,\n",
       "          0.2392,  0.5679]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c24b3",
   "metadata": {},
   "source": [
    "# Complete Class Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91cfe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# config numbers\n",
    "class ViTConfig:\n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=4,\n",
    "                 in_channels=3,\n",
    "                 emb_size=64,\n",
    "                 depth=6,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.1):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_size = emb_size\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)        # (B, emb_size, H/patch, W/patch)\n",
    "        x = x.flatten(2)              # (B, emb_size, N)\n",
    "        x = x.transpose(1, 2)         # (B, N, emb_size)\n",
    "        return x                      # Shape: (B, N, E)\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_ratio, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_size, int(emb_size * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(emb_size * mlp_ratio), emb_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Vit Test Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        cfg = config[\"model\"]\n",
    "        \n",
    "        self.CHANNEL = cfg[\"in_channels\"]\n",
    "        self.PATCH = cfg[\"patch_size\"]\n",
    "        self.EMBEDDING = cfg[\"emb_size\"]\n",
    "        self.IMAGE = cfg[\"img_size\"]\n",
    "        self.NUM_HEADS = cfg[\"num_heads\"]\n",
    "        self.MLP_RATIO = cfg[\"mlp_ratio\"]\n",
    "        self.DROPOUT = cfg[\"dropout\"]\n",
    "        self.NUM_CLASS = cfg[\"num_classes\"]\n",
    "        self.DEPTH = cfg[\"depth\"]\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_channels=self.CHANNEL,\n",
    "            patch_size=self.PATCH,\n",
    "            emb_size=self.EMBEDDING,\n",
    "            img_size=self.IMAGE\n",
    "        )\n",
    "\n",
    "        self.n_patches = (self.IMAGE// self.PATCH) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.EMBEDDING))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, self.EMBEDDING))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderBlock(\n",
    "                emb_size=self.EMBEDDING,\n",
    "                num_heads=self.NUM_HEADS,\n",
    "                mlp_ratio=self.MLP_RATIO,\n",
    "                dropout=self.DROPOUT\n",
    "            ) for _ in range(self.DEPTH)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.EMBEDDING)\n",
    "        self.head = nn.Linear(self.EMBEDDING, self.NUM_CLASS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = self.encoder(x)\n",
    "        cls_out = self.norm(x[:, 0])\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c14041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "CURR_PATH = f'/home/wd/Documents/work_stuff/ViT_REPLICATION'\n",
    "sys.path.append(os.path.abspath(CURR_PATH))  # Adds root directory to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10947e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config_loader import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2762f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': {'path': '/home/wd/Documents/work_stuff/ViT_REPLICATION'},\n",
       " 'data': {'dataset': 'CIFAR10',\n",
       "  'data_path': '/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CIFAR10',\n",
       "  'batch_size': 64,\n",
       "  'num_workers': 4,\n",
       "  'img_size': 32},\n",
       " 'model': {'img_size': 32,\n",
       "  'patch_size': 4,\n",
       "  'in_channels': 3,\n",
       "  'emb_size': 128,\n",
       "  'depth': 4,\n",
       "  'num_heads': 4,\n",
       "  'mlp_ratio': 4.0,\n",
       "  'num_classes': 10,\n",
       "  'dropout': 0.1}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading config file for CIFAR10\n",
    "config = load_config(f\"{CURR_PATH}/config/vit_test_config.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c23e9b",
   "metadata": {},
   "source": [
    "### building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(config)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "print(dummy_input.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da1fddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3500,  0.0149,  0.6415,  0.1470,  0.7542, -0.4720, -0.0573, -0.1689,\n",
       "         -1.3927,  0.0716]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846576ec",
   "metadata": {},
   "source": [
    "### loading Data CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea01113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 782, Test batches: 157\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import DatasetLoader\n",
    "\n",
    "# loading config file for CIFAR10\n",
    "data_cfg = config[\"data\"]\n",
    "DATASET = data_cfg[\"dataset\"]\n",
    "DATA_DIR = data_cfg[\"data_path\"]\n",
    "BATCH = data_cfg[\"batch_size\"]\n",
    "NUM_WORKERS = data_cfg[\"num_workers\"]\n",
    "IMAGE = data_cfg[\"img_size\"]\n",
    "\n",
    "# loading data\n",
    "loader = DatasetLoader(dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, test_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db63014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73df3f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes present in the batch of 64 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 9, 6, 7, 5, 2, 0, 2, 4, 8, 2, 5, 7, 0, 2, 4, 5, 1, 7, 6, 8, 0, 5,\n",
       "        2, 5, 0, 8, 8, 1, 5, 7, 0, 5, 5, 2, 5, 7, 0, 3, 5, 2, 8, 0, 8, 5, 1, 4,\n",
       "        3, 5, 8, 7, 3, 1, 6, 2, 1, 8, 6, 2, 5, 8, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('classes present in the batch of 64 images')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257d01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
