{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41af5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "ROOT_DIR_PATH = os.environ.get('ROOT_PATH')\n",
    "sys.path.append(os.path.abspath(ROOT_DIR_PATH))  # Adds root directory to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59737e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config\n",
      "loading dataset : CALTECH256\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/CALTECH256 already exists, zip downloaded.\n",
      "Dataset directory /home/wd/Documents/work_stuff/ViT_REPLICATION/data/CALTECH256 already exists, zip downloaded.\n",
      "training size  : 100\n",
      "validation size : 6056\n",
      "Subset contains 10 unique classes\n",
      "Sample label: 0\n",
      "Train batches: 1, Validation batches: 48\n",
      "data sanity check\n",
      "image shape and labels shape in training data - one batch : torch.Size([100, 3, 224, 224]), torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import DatasetLoader\n",
    "from utils.config_loader import load_config\n",
    "print('loading config')\n",
    "# Load config\n",
    "config = load_config(f\"{ROOT_DIR_PATH}/config/vit_config.yaml\")\n",
    "\n",
    "# *************  choosing the DATASET & MODEL *************\n",
    "\n",
    "dataset_config = config[\"data\"]['CALTECH256']\n",
    "trainingConfig = config['training_dummy']\n",
    "\n",
    "# **********************************************************\n",
    "\n",
    "# data\n",
    "DATASET = dataset_config[\"dataset\"]\n",
    "DATA_DIR =f'{ROOT_DIR_PATH}/data/{DATASET}/'\n",
    "BATCH = dataset_config[\"batch_size\"]\n",
    "NUM_WORKERS = dataset_config[\"num_workers\"]\n",
    "IMAGE = dataset_config[\"img_size\"]\n",
    "NUM_CLASSES = dataset_config[\"num_classes\"]\n",
    "CHANNELS = dataset_config[\"channels\"]\n",
    "if DATASET == 'TINYIMAGENET200':\n",
    "    SUBSET_ENABLED = dataset_config['subset_enabled']\n",
    "    SUBSET_SIZE = dataset_config['subset_size']\n",
    "\n",
    "# loading data\n",
    "print(f'loading dataset : {DATASET}')\n",
    "loader = DatasetLoader(training_config=trainingConfig,\n",
    "                        dataset_name=DATASET,\n",
    "                        data_dir=DATA_DIR,\n",
    "                        batch_size=BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        img_size=IMAGE)\n",
    "train_loader, val_loader = loader.get_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print('data sanity check')\n",
    "for images, labels in train_loader:\n",
    "    print(f'image shape and labels shape in training data - one batch : {images.shape}, {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 5.00%\n",
      "Epoch 02 | Train Acc: 20.00%\n",
      "Epoch 03 | Train Acc: 12.00%\n",
      "Epoch 04 | Train Acc: 21.00%\n",
      "Epoch 05 | Train Acc: 31.00%\n",
      "Epoch 06 | Train Acc: 33.00%\n",
      "Epoch 07 | Train Acc: 34.00%\n",
      "Epoch 08 | Train Acc: 36.00%\n",
      "Epoch 09 | Train Acc: 35.00%\n",
      "Epoch 10 | Train Acc: 35.00%\n",
      "Epoch 11 | Train Acc: 35.00%\n",
      "Epoch 12 | Train Acc: 39.00%\n",
      "Epoch 13 | Train Acc: 46.00%\n",
      "Epoch 14 | Train Acc: 41.00%\n",
      "Epoch 15 | Train Acc: 43.00%\n",
      "Epoch 16 | Train Acc: 46.00%\n",
      "Epoch 17 | Train Acc: 46.00%\n",
      "Epoch 18 | Train Acc: 49.00%\n",
      "Epoch 19 | Train Acc: 48.00%\n",
      "Epoch 20 | Train Acc: 55.00%\n",
      "Epoch 21 | Train Acc: 52.00%\n",
      "Epoch 22 | Train Acc: 55.00%\n",
      "Epoch 23 | Train Acc: 58.00%\n",
      "Epoch 24 | Train Acc: 56.00%\n",
      "Epoch 25 | Train Acc: 59.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 25\n",
    "LR = 0.001\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# ========== Model ==========\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = vit_tiny_patch16_224(pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7494f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wd/Documents/work_stuff/ViT_REPLICATION/_vit_rep_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 8.00%\n",
      "Epoch 02 | Train Acc: 18.00%\n",
      "Epoch 03 | Train Acc: 25.00%\n",
      "Epoch 04 | Train Acc: 32.00%\n",
      "Epoch 05 | Train Acc: 36.00%\n",
      "Epoch 06 | Train Acc: 43.00%\n",
      "Epoch 07 | Train Acc: 50.00%\n",
      "Epoch 08 | Train Acc: 60.00%\n",
      "Epoch 09 | Train Acc: 77.00%\n",
      "Epoch 10 | Train Acc: 80.00%\n",
      "Epoch 11 | Train Acc: 94.00%\n",
      "Epoch 12 | Train Acc: 91.00%\n",
      "Epoch 13 | Train Acc: 94.00%\n",
      "Epoch 14 | Train Acc: 96.00%\n",
      "Epoch 15 | Train Acc: 97.00%\n",
      "Epoch 16 | Train Acc: 99.00%\n",
      "Epoch 17 | Train Acc: 100.00%\n",
      "Model has successfully overfitted the subset.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from timm.models.vision_transformer import vit_tiny_patch16_224\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "gc.collect()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 25\n",
    "LR = 0.001\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# ========== Model ==========\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TwoLayerCNN(nn.Module):\n",
    "    def __init__(self, num_classes, img_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B, 32, 32, 32)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (B, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B, 64, 16, 16)\n",
    "            nn.Flatten(),  # (B, 64*16*16)\n",
    "            nn.Linear(64 * (img_size // 4) * (img_size // 4), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "model = TwoLayerCNN(num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(DEVICE)\n",
    "\n",
    "# ========== Loss & Optimizer ==========\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ========== Train Loop ==========\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "    # stop early if we hit 100% train acc\n",
    "    if acc == 100.0:\n",
    "        print(\"Model has successfully overfitted the subset.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a964cc",
   "metadata": {},
   "source": [
    "# Mean / STD DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25f8928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean/std: 100%|██████████| 742/742 [00:32<00:00, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Mean: [0.5531906485557556, 0.5342377424240112, 0.5071621537208557]\n",
      "Dataset Std : [0.23687367141246796, 0.2358572781085968, 0.2385127693414688]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your Caltech256 training set (change if needed)\n",
    "data_path = \"/home/wd/Documents/work_stuff/ViT_REPLICATION/data/CALTECH256/train\"\n",
    "\n",
    "# Resize and convert to tensor (no normalization yet)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a fixed size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize sums\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "\n",
    "# Accumulate mean and std across batches\n",
    "for data, _ in tqdm(loader, desc=\"Computing mean/std\"):\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)  # [B, C, H*W]\n",
    "    mean += data.mean(2).sum(0)  # Sum of mean across batch\n",
    "    std += data.std(2).sum(0)    # Sum of std across batch\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "# Final average\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print(f\"\\nDataset Mean: {mean.tolist()}\")\n",
    "print(f\"Dataset Std : {std.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_vit_rep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
